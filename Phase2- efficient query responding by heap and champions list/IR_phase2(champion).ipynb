{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import datetime\n",
    "import collections\n",
    "import math\n",
    "import heapq\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## marks lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_marks = [')','(','>','<',\"؛\",\"،\",'{','}',\"؟\",':',\"-\", '»', '\"', '«', '[', ']','\"','+','=','?']\n",
    "marks = ['/','//', '\\\\','|','!', '%', '&','*','$', '#','؟', '*','.','_' ]\n",
    "alphabet_string_lower = string.ascii_lowercase\n",
    "alphabet_string_upper = string.ascii_uppercase\n",
    "english_char =  list(alphabet_string_lower) + list(alphabet_string_upper)\n",
    "\n",
    "\n",
    "sep_list = [\" \", '\\xad', '\\u200e','\\u200f', '\\u200d', '\\u200d', '\\u200d'] + marks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regex  patterns, prefixe list and postfix list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "raw_postfix = [\n",
    "        ('[\\u200c](تر(ین?)?|گری?|های?)' , ''),\n",
    "        (r'(تر(ین?)?|گری?)(?=$)' , ''), #تر ، ترین، گر، گری در آخر کلمه\n",
    "        (r'(?<=[^ او])(م|ت|ش|مان|تان|شان)$' , ''), # حذف شناسه های مالکیت و فعل در آخر کلمه\n",
    "        (r'(ان|ات|گی|گری|های)$' , ''), #ان، ات، ها، های آخر کلمه  \n",
    "]\n",
    "\n",
    "raw_arabic_notation = [\n",
    "    # remove FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA, KASRA, SHADDA, SUKUN\n",
    "    ('[\\u064B\\u064C\\u064D\\u064E\\u064F\\u0650\\u0651\\u0652]', ''),\n",
    "    ]\n",
    "\n",
    "raw_long_letters = [\n",
    "    (r' +', ' '),  # remove extra spaces\n",
    "    (r'\\n\\n+', '\\n'),  # remove extra newlines\n",
    "    (r'[ـ\\r]', ''),  # remove keshide, carriage returns\n",
    "    ]\n",
    "\n",
    "raw_half_space = [\n",
    "    ('[\\u200c]', ''),\n",
    "]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verbs, mokassar,morakab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_roots =['توان','باش','رو','بر','یاور', 'یانداز', 'یای','یاندیش','بخش','باز','خر','بین','شنو','دار','دان','رسان','شناس','گو','گذار','یاب','لرز','ساز','شو','نویس','خوان','کاه','گیر','خواه','کن' ]\n",
    "\n",
    "past_roots = ['توانست','بود','کرد','اورد','انداخت','امد','خرید','باخت','برد','رفت','اندیشید','بخشید','دید','شنید','داشت','دانست','رساند','شناخت','گفت','گذشت','یافت','لرزید','ساخت','شد','نوشت','خواند','کاست','گرفت','خواست']\n",
    "all_verbs_roots = present_roots + past_roots\n",
    "empty_list = ['','','','','','','']\n",
    "verb_prefix = ['نمی‌', 'می‌','ن','ب',\"\" ]\n",
    "present_verb_postfix = ['م','ی','د','ید','ند','یم']\n",
    "past_verb_postfix = ['ایم','اید','ای','ام','اند']\n",
    "past_verb_postfix2 = ['م','ی','ید','ند','یم']\n",
    "present_verbs = []\n",
    "past_verbs = []\n",
    "all_verbs = {}\n",
    "for pref in verb_prefix:\n",
    "    for present_root, past_root in zip(present_roots, past_roots):\n",
    "        for post in past_verb_postfix2:\n",
    "            all_verbs[pref + past_root+post] = past_root\n",
    "        for post in past_verb_postfix:\n",
    "            all_verbs[past_root+'ه‌'+post] = past_root\n",
    "        for post in present_verb_postfix:\n",
    "            all_verbs[pref+present_root+post] = present_root\n",
    "            \n",
    "words_list = ['ادب', 'آداب', 'طرف', 'اطراف', 'حقیقت', 'حقایق', 'موج', 'امواج', 'ادیب', 'ادبا', 'عمق', 'اعماق', 'خزینه', 'خزائن', 'مرکز', 'مراکز', 'اثر', 'آثار', 'عالم', 'علما', 'خبر', 'اخبار', 'موقع', 'مواقع', 'اسیر', 'اسرا', 'علم', 'علوم', 'دوره', 'ادوار', 'مصرف', 'مصارف', 'اسم', 'اسامی', 'علامت', 'علائم', 'دین', 'ادیان', 'معرفت', 'معارف', 'اسطوره', 'اساطیر', 'علت', 'علل', 'دفتر', 'دفاتر', 'مبحث', 'مباحث', 'امیر', 'امرا', 'عقیده', 'عقائد', 'ذخیره', 'ذخایر', 'ماده', 'مواد', 'امر', 'اوامر', 'عمل', 'اعمال', 'رفیق', 'رفقا', 'مذهب', 'مذاهب', 'امام', 'ائمه', 'عید', 'اعیاد', 'رای', 'آرا', 'مصیبت', 'مصائب', 'اصل', 'اصول', 'عنصر', 'عناصر', 'رسم', 'رسوم', 'معبد', 'معابد', 'افق', 'آفاق', 'عاطفه', 'عواطف', 'رابطه', 'روابط', 'مسجد', 'مساجد', 'بیت', 'ابیات', 'عضو', 'اعضا', 'رمز', 'رموز', 'معبر', 'معابر', 'تاجر', 'تجار', 'عبارت', 'عبارات', 'رجل', 'رجال', 'مظهر', 'مظاهر', 'تصویر', 'تصاویر', 'عجیب', 'عجایب', 'رقم', 'ارقام', 'منظره', 'مناظر', 'جد', 'اجداد', 'فقیه', 'فقها', 'زاویه', 'زوایا', 'مرض', 'امراض', 'جانب', 'جوانب', 'فن', 'فنون', 'زعیم', 'زعما', 'مورد', 'موارد', 'جزیره', 'جزایر', 'فکر', 'افکار', 'سانحه', 'سوانح', 'مرحله', 'مراحل', 'جبل', 'جبال', 'فریضه', 'فرایض', 'سلطان', 'سلاطین', 'مفهوم', 'مفاهیم', 'جریمه', 'جرایم', 'فعل', 'افعال', 'شعر', 'اشعار', 'منبع', 'منابع', 'حادثه', 'حوادث', 'فقیر', 'فقرا', 'شاعر', 'شعرا', 'مکان', 'اماکن', 'حشم', 'احشام', 'قاعده', 'قواعد']\n",
    "mokassar_dict ={}\n",
    "for i in range(0,len(words_list),2):\n",
    "    mokassar_dict[words_list[i+1]] = words_list[i]\n",
    "                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(txt, seps):\n",
    "    default_sep = seps[0]    \n",
    "    # we skip seps[0] because that's the default separator\n",
    "    for sep in seps[1:]:\n",
    "        txt = txt.replace(sep, default_sep)\n",
    "    return [i.strip() for i in txt.split(default_sep)]\n",
    "\n",
    "#get doc and return (word,doc id)\n",
    "def tokenizer(text):\n",
    "    for l in english_char:\n",
    "        text = text.replace(l,\"\")\n",
    "    word_list = split(text.strip().replace(\"\\n\",\" \"), sep_list)\n",
    "    word_list = [x for x in word_list if not x.startswith(\"ir\") and not x.startswith(\"NID\")]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_marks(word_list):\n",
    "#     word_list = list(word_list)\n",
    "    for i in range(len(word_list)):\n",
    "        for mark in punctuation_marks:\n",
    "            word_list[i] = word_list[i].replace(mark,\"\")\n",
    "\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def edit_long_letters(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_long_letters]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_arabic_notation(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_arabic_notation]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def create_translation_table(src_list, dst_list):\n",
    "     return dict((ord(a), b) for a, b in zip(src_list, dst_list)) #map src unicode to dst char\n",
    "\n",
    "    \n",
    "def char_digit_Unification(word_list): \n",
    "    word_list = [x for x in word_list if not x.isnumeric() or (x.isnumeric() and float(x)<3000)]\n",
    "    translation_src, translation_dst = ' ىكي“”', ' یکی\"\"'\n",
    "    translation_src += 'ئ0123456789أإآ%'\n",
    "    translation_dst += 'ی۰۱۲۳۴۵۶۷۸۹ااا٪'\n",
    "    translations_table = create_translation_table(translation_src, translation_dst)\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i].translate(translations_table)\n",
    "    return word_list\n",
    "\n",
    "def remove_mokassar(word_list):\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = mokassar_dict.get(word_list[i],word_list[i])\n",
    "        \n",
    "    return word_list\n",
    "    \n",
    "\n",
    "\n",
    "def verb_Steaming(word_list):\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = all_verbs.get(word_list[i],word_list[i])\n",
    "        \n",
    "    return word_list\n",
    "    \n",
    "\n",
    "def remove_postfix(word_list):\n",
    "\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_postfix]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            if len(word_list[i])>4 and  not (word_list[i] in all_verbs_roots):\n",
    "                word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def remove_prefix(word_list):\n",
    "    starts =['بی‌' , 'نا', 'با']\n",
    "    for i in range(len(word_list)):\n",
    "        for start in starts:\n",
    "            if word_list[i].startswith(start) and len(word_list[i])>4:\n",
    "                word_list[i] = word_list[i][len(start):]\n",
    "                break\n",
    "\n",
    "    return word_list\n",
    "\n",
    "def morakab_Unification(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_half_space]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "#all to gether \n",
    "def normalizer(word_list):\n",
    "    #remove punc\n",
    "    for i in range(len(word_list)):\n",
    "        for mark in punctuation_marks:\n",
    "            word_list[i] = word_list[i].replace(mark,\"\")\n",
    "            \n",
    "    #edit long letters\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_long_letters]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "            \n",
    "    #remove mokassar\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = mokassar_dict.get(word_list[i],word_list[i])\n",
    "    \n",
    "    #remove_arabic_notation\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_arabic_notation]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    \n",
    "    #char_digit_Unification\n",
    "    word_list = [x for x in word_list if not x.isnumeric() or (x.isnumeric() and float(x)<3000)]\n",
    "    translation_src, translation_dst = ' ىكي“”', ' یکی\"\"'\n",
    "    translation_src += 'ئ0123456789أإآ%'\n",
    "    translation_dst += 'ی۰۱۲۳۴۵۶۷۸۹ااا٪'\n",
    "    translations_table = create_translation_table(translation_src, translation_dst)\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i].translate(translations_table)\n",
    "    \n",
    "    #verb_Steaming\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = all_verbs.get(word_list[i],word_list[i])\n",
    "        \n",
    "    #remove_prefix\n",
    "    starts =['بی‌' , 'نا', 'با']\n",
    "    for i in range(len(word_list)):\n",
    "        for start in starts:\n",
    "            if word_list[i].startswith(start) and len(word_list[i])>4:\n",
    "                word_list[i] = word_list[i][len(start):]\n",
    "                break\n",
    "    \n",
    "    #remove_postfix\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_postfix]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            if len(word_list[i])>4 and  not (word_list[i] in all_verbs_roots):\n",
    "                word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    #morakab_Unification\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_half_space]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 855 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cols = ['id']\n",
    "df = pd.read_excel('IR_Spring2021_ph12_7k.xlsx', engine = 'openpyxl')\n",
    "contents = df['content'].to_list()\n",
    "doc_ids = df['id']\n",
    "doc_url = df['url']\n",
    "docs_num = len(doc_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_champion_list(inverted_index, r):\n",
    "    for term in inverted_index.keys():\n",
    "        doc_freq_dict = inverted_index[term][\"normal\"]\n",
    "        heap = [(-value, key) for key,value in doc_freq_dict.items()]\n",
    "        largest = heapq.nsmallest(r, heap)\n",
    "        champion_list = { y:-x for x,y in largest}\n",
    "        champion_list = sorted(champion_list.items(), key=lambda kv: kv[0])\n",
    "        inverted_index[term][\"champion\"] = {key:value for key, value in champion_list}\n",
    "    \n",
    "\n",
    "def create_dictionary(contents, doc_ids, r):\n",
    "    for content, id in zip(contents,doc_ids):\n",
    "        doc_tokens = tokenizer(content)\n",
    "        doc_normalized_tokens = normalizer(doc_tokens)\n",
    "        term_freq_dict = dict(collections.Counter(doc_normalized_tokens))\n",
    "        # we replace contents by bag of word style doc : { term -> freq}\n",
    "        contents[id-1] = term_freq_dict\n",
    "        for term,freq in term_freq_dict.items():\n",
    "            term_postings = inverted_index.get(term,{\"normal\":{}, \"champion\":{}})\n",
    "            term_postings[\"normal\"][id] = freq\n",
    "#             term_postings[\"champion\"][id] = freq\n",
    "            inverted_index[term] = term_postings\n",
    "    # term -> {normal : list of docs, champion: list of champion docs(length of the lis is r)}\n",
    "    create_champion_list(inverted_index, r)\n",
    "    \n",
    "\n",
    "def delete_soptwords(inverted_index):\n",
    "    stop_words = []\n",
    "    for key in list(inverted_index):\n",
    "        if len(inverted_index[key][\"normal\"])> 0.25* docs_num or len(key) <2: \n",
    "            stop_words.append(key)\n",
    "            del inverted_index[key]\n",
    "    return stop_words\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words:  ['به', 'گزار', 'ایسنا', 'از', 'ایر', 'با', 'این', 'هم', 'داد', '', 'که', 'شده', 'حضور', 'را', 'در', 'های', 'کن', 'است', 'سال', 'خود', 'کرد', 'عنو', 'بود', 'باش', 'اعلام', 'می', 'بر', 'و', 'شد', 'نیز', 'برای', 'خواه', 'کشور', 'روز', 'همچنین', 'کرده', 'اینکه', '۳', 'انت', 'پیام', 'بیان', 'شو', 'گفت', 'توان', 'وی', 'افزود', 'ساز', 'توجه', 'اظهار', 'تا', '۲', 'داشت', 'باید', 'مورد', 'انجام', 'دار', 'گیر', 'دو', 'هر', 'ان', 'رو', 'یا', 'صورت', 'یک', 'ی', 'اما', '٢', 'گذشته', 'هزار', '۴', 'قرار', 'بیش', 'دیگر', 'ادامه', 'امروز', '۰', '۹', '۷', '۵', '۶', '۱', '۸', 'ا', '٣', 'ب', 'س', '–', '٨', 'ج', '٧', 'پ', \"'\", 'ز', 'ر', 'ه', 'ع', 'ء', 'م', '\\u202c', 'ش', '٦', 'ن', '٩', '@', 'ۚ', '٤', 'د', 'ص', 'ک', '\\u202b', 'ث', 'چ', 'ط', 'ح', 'ل', 'ف', 'ت', '…', '٥', '\\u2067', '١', 'ذ', 'ق', '—', '٠', ',', 'غ', '\\ufeff', '٪', '\"', '\\u2069', '•']\n",
      "Wall time: 23.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inverted_index ={}\n",
    "create_dictionary(contents, doc_ids, 200)\n",
    "stop_words = delete_soptwords(inverted_index)\n",
    "print(\"stop words: \", stop_words )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# print(len(inverted_index['ملی'][\"champion\"]))\n",
    "# print(((inverted_index['ملی'][\"champion\"])))\n",
    "# print(\"/*****************\")\n",
    "# print((inverted_index['ملی'][\"normal\"]))\n",
    "print(len(inverted_index['سلام']['champion']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_inverted_index = open('ph2_inverted_index.obj', 'wb') \n",
    "pickle.dump(inverted_index, file_inverted_index)\n",
    "file_inverted_index.close()\n",
    "inverted_index = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = open('ph2_contents.obj', 'wb') \n",
    "pickle.dump(contents, file_contents)\n",
    "file_contents.close()\n",
    "contents =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 426 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "readed = open('ph2_inverted_index.obj', 'rb') \n",
    "inverted_index = pickle.load(readed)\n",
    "readed = open('ph2_contents.obj', 'rb') \n",
    "contents = pickle.load(readed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorize query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_query(query_str):\n",
    "    # free_text query => {term: tf}\n",
    "    query_tokens = tokenizer(query_str)\n",
    "    query_normalized_tokens = normalizer(query_tokens)\n",
    "    print(query_normalized_tokens)\n",
    "    term_freq_dict = dict(collections.Counter(query_normalized_tokens))\n",
    "    query_terms = {}\n",
    "    for  term,freq in term_freq_dict.items():\n",
    "        if term in inverted_index.keys() and term not in stop_words:\n",
    "            query_terms[term] = 1+math.log(freq,10)\n",
    "    return query_terms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query index elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_docs(query_terms, postings_type = \"normal\"):\n",
    "    # method parameters define we should iterate on which postings list\n",
    "    docs_score = [0] * (docs_num+1)\n",
    "    # we use TAAT method and score all docs which have the term on each iterate \n",
    "    for term,Wt_q in query_terms.items():\n",
    "        idf = math.log(docs_num/len(inverted_index[term]['normal']))\n",
    "        if postings_type == \"normal\":\n",
    "            for doc_id, freq in inverted_index[term][\"normal\"].items():\n",
    "                docs_score[doc_id] += Wt_q * (1+math.log(freq,10)) * idf\n",
    "        elif postings_type == \"champion\":\n",
    "            for doc_id, freq in inverted_index[term][\"champion\"].items():\n",
    "                docs_score[doc_id] += Wt_q * (1+math.log(freq,10)) * idf\n",
    "    return docs_score\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_length(doc_id):\n",
    "    sum =0\n",
    "    for term, freq in contents[doc_id-1].items():\n",
    "        if term not in stop_words:\n",
    "            # sum += (tf * idf)**2\n",
    "            sum += ((1+(math.log(freq,10))) * math.log(docs_num/len(inverted_index[term]['normal']),10) )**2\n",
    "    return math.sqrt(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(docs_score):\n",
    "    docs_similarity = {}\n",
    "    for doc_id in range(1,docs_num+1):\n",
    "        if docs_score[doc_id] != 0:\n",
    "            docs_similarity[doc_id] = docs_score[doc_id]/calculate_length(doc_id)\n",
    "    return docs_similarity\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_index_elimination(query_str, postings_type=\"normal\" ):\n",
    "    query_terms = vectorize_query(query_str)\n",
    "    docs_score = score_docs(query_terms, postings_type)\n",
    "    docs_similarity = similarity(docs_score)\n",
    "    return docs_similarity \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top-k extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_k(docs_similarity, k, method = \"heap\"):\n",
    "    #heap method\n",
    "    if method == \"heap\": \n",
    "        heap = [(-value, key) for key,value in docs_similarity.items()]\n",
    "        largest = heapq.nsmallest(k, heap)\n",
    "        top_k = [(y, -x) for x,y in largest]\n",
    "        return top_k\n",
    "    \n",
    "    # sort method\n",
    "    else:\n",
    "        docs_similarity_list = list(docs_similarity.items())\n",
    "        for mx in range(len(docs_similarity_list)-1, -1, -1):\n",
    "            swapped = False\n",
    "            for i in range(mx):\n",
    "                if docs_similarity_list[i][1] < docs_similarity_list[i+1][1]:\n",
    "                    docs_similarity_list[i], docs_similarity_list[i+1] = docs_similarity_list[i+1], docs_similarity_list[i]\n",
    "                    swapped = True\n",
    "            if not swapped:\n",
    "                break\n",
    "        top_k = docs_similarity_list[:k]\n",
    "        return top_k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query responding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_responding(postings_type, top_k_method):\n",
    "    query = input(\"enter query: \")\n",
    "    a = datetime.datetime.now()\n",
    "    docs_similarity = query_index_elimination(query, postings_type)\n",
    "    top_docs = extract_top_k(docs_similarity, 50, top_k_method )\n",
    "    b = datetime.datetime.now()\n",
    "\n",
    "\n",
    "    print(\"{:<5} result in {} ms\\nid \\t(score)\\t\\t  -> link\\n\".format(len(top_docs),1000*(b-a).total_seconds()))\n",
    "    for doc_id, score in top_docs:\n",
    "        print( \"{:<5}({}) -> {} \".format(doc_id,score, doc_url[doc_id-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter query: تحرک ذهنی زنان خانه‌دار\n",
      "['تحرک', 'ذهنی', 'زنان', 'خانهدار']\n",
      "50    result in 147.593 ms\n",
      "id \t(score)\t\t  -> link\n",
      "\n",
      "6718 (1.0697567199327356) -> https://www.isna.ir/news/98042915171/زنان-شاغل-دیرتر-به-آلزایمر-مبتلا-می-شوند \n",
      "6722 (1.0697567199327356) -> https://www.isna.ir/news/98042915171/زنان-شاغل-دیرتر-به-آلزایمر-مبتلا-می-شوند \n",
      "1606 (0.5457466377911007) -> https://www.isna.ir/news/98111309170/سفر-تیم-ملی-فوتسال-بانوان-چین-به-ایران-لغو-شد \n",
      "2733 (0.46947215051185165) -> https://www.isna.ir/news/98042614092/تحرک-نیروی-زمینی-نسبت-به-سال-گذشته-30-درصد-افزایش-داشته-است \n",
      "1921 (0.4686017778103753) -> https://www.isna.ir/news/99050100921/جلسه-شورای-مرکزی-مجمع-زنان-اصلاح-طلب-برگزار-شد \n",
      "3038 (0.46605868065945855) -> https://www.isna.ir/news/98091309931/بسیج-برنامه-انتخاباتی-برای-مجلس-ندارد \n",
      "6921 (0.4593294302330212) -> https://www.isna.ir/news/98081308061/حواس-افسردگی-را-پرت-کنید \n",
      "6924 (0.4593294302330212) -> https://www.isna.ir/news/98081308061/حواس-افسردگی-را-پرت-کنید \n",
      "6928 (0.4593294302330212) -> https://www.isna.ir/news/98081308061/حواس-افسردگی-را-پرت-کنید \n",
      "2067 (0.4589869079311758) -> https://www.isna.ir/news/99062115929/رایزنی-ظریف-با-وزیر-امور-خارجه-سنت-وینسنت \n",
      "734  (0.45132006618474) -> https://www.isna.ir/news/99091209729/عموم-جامعه-ورزش-های-الکترونیک-را-تفریح-و-وقت-گذرانی-تلقی-می-کنند \n",
      "6207 (0.44052307257830703) -> https://www.isna.ir/news/99082819819/راه-های-پیشگیری-از-دیابت-را-بهتر-بشناسیم \n",
      "2913 (0.3919895844215327) -> https://www.isna.ir/news/98071511860/انتخاب-اعضای-هیات-رییسه-جدید-فراکسیون-زنان-مجلس \n",
      "797  (0.3906128922202762) -> https://www.isna.ir/news/99100604215/انتقاد-حسن-یزدانی-از-نحوه-پرداخت-پاداش-قهرمانان-وزیر-ورزش-چاره \n",
      "1907 (0.38136069773081904) -> https://www.isna.ir/news/99042317334/تاکید-فتاح-بر-آمادگی-بنیاد-مستضعفان-برای-آزادی-زنان-زندانی-سرپرست \n",
      "1815 (0.37585551878250445) -> https://www.isna.ir/news/99022719270/جلسه-شورای-مرکزی-جمعیت-زنان-مسلمان-نواندیش-برگزار-شد \n",
      "756  (0.37470219613085254) -> https://www.isna.ir/news/99091814045/تیروکمان-بدون-تغییر-در-المپیک-۲۰۲۴-پاریس \n",
      "5905 (0.3717123699051994) -> https://www.isna.ir/news/99052719954/جشن-۱۹-سالگی-باشگاه-هنرمندان-ایران-در-حمایت-از-کادر-درمان \n",
      "2829 (0.3666550348112389) -> https://www.isna.ir/news/98060301436/رفع-ایرادات-شورای-نگهبان-به-لایحه-تابعیت-فرزندان-در-کمیسیون-حقوقی \n",
      "1698 (0.36202483397220253) -> https://www.isna.ir/news/98121713481/لیگ-تکواندوی-زنان-و-مردان-لغو-شد \n",
      "6863 (0.35931380270290986) -> https://www.isna.ir/news/98070805903/فریب-نمک-دریا-را-نخورید \n",
      "873  (0.3548929242611574) -> https://www.isna.ir/news/99110201452/تاریخ-برگزاری-مسابقات-هندبال-قهرمانی-زنان-آسیا-مشخص-شد \n",
      "972  (0.3480026289872133) -> https://www.isna.ir/news/99120402801/سومی-فولاد-مبارکه-سپاهان-در-لیگ-برتر-هندبال-زنان \n",
      "1892 (0.340277565244663) -> https://www.isna.ir/news/99041511345/ساماندهی-بازار-مسکن-در-کمیسیون-عمران-مجلس-کلید-خورد \n",
      "3529 (0.340277565244663) -> https://www.isna.ir/news/99041511345/ساماندهی-بازار-مسکن-در-کمیسیون-عمران-مجلس-کلید-خورد \n",
      "815  (0.3380076372929467) -> https://www.isna.ir/news/99101410460/عزل-و-نصب-رئیس-هیات-زورخانه-ای-تهران-در-آخرین-روز-ریاست-جوهری \n",
      "2284 (0.33749348947686036) -> https://www.isna.ir/news/99090403542/تاکید-احزاب-زنان-اصلاح-طلب-بر-تفسیر-شفاف-رجل-سیاسی-از-سوی-شورای \n",
      "32   (0.3320386816405065) -> https://www.isna.ir/news/99011105253/بهترین-حرکات-ورزشی-در-قرنطینه-خانگی-تصویر \n",
      "6908 (0.32735949764705713) -> https://www.isna.ir/news/98080402002/مولفه-های-نشاط-اجتماعی-چیست \n",
      "6913 (0.32735949764705713) -> https://www.isna.ir/news/98080402002/مولفه-های-نشاط-اجتماعی-چیست \n",
      "6917 (0.32735949764705713) -> https://www.isna.ir/news/98080402002/مولفه-های-نشاط-اجتماعی-چیست \n",
      "597  (0.32255205304303075) -> https://www.isna.ir/news/99072820710/برتری-پرسپولیس-بهبهان-در-لیگ-برتر-هندبال-زنان \n",
      "819  (0.32230774126252654) -> https://www.isna.ir/news/99101612434/مس-رفسنجان-در-جمع-۱۰-تیم-برتر-فوتسال-بانوان-جهان-در-سال۲۰۲۰ \n",
      "6396 (0.31866766136983093) -> https://www.isna.ir/news/99111712509/انتشار-مجموعه-قوانین-در-ارتباط-با-سلامت-زنان-در-ایران \n",
      "785  (0.31767457822738343) -> https://www.isna.ir/news/99100100457/صعود-تیم-خادم-الشریعه-به-مرحله-نهایی-مسابقات-آنلاین-جام-باشگاه-های \n",
      "6974 (0.3176105223405714) -> https://www.isna.ir/news/98091107750/از-بین-رفتن-۱۰-درصد-بافت-عضلانی-با-استراحت-مطلق-عدم-تناسب-۹۰ \n",
      "6979 (0.3176105223405714) -> https://www.isna.ir/news/98091107750/از-بین-رفتن-۱۰-درصد-بافت-عضلانی-با-استراحت-مطلق-عدم-تناسب-۹۰ \n",
      "1524 (0.31022379278288664) -> https://www.isna.ir/news/98093022796/توضیحات-معاون-بانوان-وزارت-ورزش-در-مورد-یک-مصاحبه \n",
      "1886 (0.30854485284653543) -> https://www.isna.ir/news/99041511390/مشروح-بررسی-های-جلسه-امروز-کمیسیون-فرهنگی-مجلس \n",
      "5781 (0.307030361780918) -> https://www.isna.ir/news/99042417626/روزشمار-هفته-حمایت-از-بیماران-هموفیلی-اعلام-شد \n",
      "727  (0.3058283312504706) -> https://www.isna.ir/news/99090907204/لیگ-برتر-هندبال-زنان-شکست-اشتادسازه-در-خانه \n",
      "2665 (0.3052044847665169) -> https://www.isna.ir/news/98032511615/موضوع-آمار-در-کشور-مغفول-باقی-مانده-است \n",
      "1719 (0.3049217651181356) -> https://www.isna.ir/news/98122620176/اعلام-تاریخ-جدید-تکواندوی-گزینشی-المپیک-در-قاره-آسیا \n",
      "6053 (0.3042369940720868) -> https://www.isna.ir/news/99070603813/ورود-۶-هزار-واکسن-آنفلوآنزا-به-استان-بوشهر \n",
      "607  (0.30386830625755834) -> https://www.isna.ir/news/99080100388/یک-پیروزی-و-یک-تساوی-در-روز-چهارم-لیگ-برتر-هندبال-زنان \n",
      "600  (0.3027137424547642) -> https://www.isna.ir/news/99072921619/شکست-ذوب-آهن-مقابل-بافق-در-لیگ-برتر-هندبال-زنان \n",
      "6737 (0.3000012571324527) -> https://www.isna.ir/news/98050904827/چگونه-رخدادهای-استرس-زا-باعث-سرطان-می-شوند \n",
      "6740 (0.3000012571324527) -> https://www.isna.ir/news/98050904827/چگونه-رخدادهای-استرس-زا-باعث-سرطان-می-شوند \n",
      "6742 (0.3000012571324527) -> https://www.isna.ir/news/98050904827/چگونه-رخدادهای-استرس-زا-باعث-سرطان-می-شوند \n",
      "5632 (0.29866388224212637) -> https://www.isna.ir/news/99030904766/با-وسواس-های-فکری-در-دوران-کرونا-چه-کنیم \n"
     ]
    }
   ],
   "source": [
    "query_responding(\"champion\",\"heap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_responding(\"champion\",\"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter query: آلودگی هوا محیط زیست\n",
      "['الود', 'هوا', 'محیط', 'زیست']\n",
      "50    result in 289.928 ms\n",
      "id \t(score)\t\t  -> link\n",
      "\n",
      "4166 (-) -> https://www.isna.ir/news/99110906805/آلایندگی-شدید-یکی-از-پالایشگاه-های-عسلویه-به-دلیل-قطعی-برق \n",
      "3583 (-) -> https://www.isna.ir/news/99042820529/محکومیت-سه-پالایشگاه-گازی-به-علت-آلودگی-خلیج-فارس \n",
      "4704 (-) -> https://www.isna.ir/news/98042513336/شکایت-محیط-زیست-از-پالایشگاهی-در-جنوب-بوشهر \n",
      "833  (-) -> https://www.isna.ir/news/99102116054/در-چه-صورتی-شهرآورد-پایتخت-لغو-می-شود \n",
      "4240 (-) -> https://www.isna.ir/news/99120503925/تلف-شدن-ماهی-ها-در-سواحل-عسلویه \n",
      "4380 (-) -> https://www.isna.ir/news/98020402451/قیر-حکم-طلا-پیدا-کرده \n",
      "4099 (-) -> https://www.isna.ir/news/99101410310/سوخت-توزیع-شده-در-تهران-دارای-استاندارد-های-لازم-است \n",
      "1321 (-) -> https://www.isna.ir/news/98060603293/شکایت-از-دوچرخه-سواری-در-لندن-بالا-گرفته \n",
      "4126 (-) -> https://www.isna.ir/news/99102418523/بلایی-که-پتروشیمی-ایلام-بر-سر-هوا-آورده-است \n",
      "6281 (-) -> https://www.isna.ir/news/99092620747/۹۰-درصد-مردم-جهان-در-معرض-هوای-آلوده-هستند \n",
      "6286 (-) -> https://www.isna.ir/news/99092620747/۹۰-درصد-مردم-جهان-در-معرض-هوای-آلوده-هستند \n",
      "4478 (-) -> https://www.isna.ir/news/98022111099/چرا-شرکت-البرزی-مالیات-به-تهران-می-دهد \n",
      "4102 (-) -> https://www.isna.ir/news/99101511182/استفاده-از-سوخت-مازوت-در-نیروگاه-های-اصفهان-قطعی-است \n",
      "1862 (-) -> https://www.isna.ir/news/99040100792/ساداتی-نژاد-رییس-کمیسیون-کشاورزی-مجلس-شد \n",
      "3488 (-) -> https://www.isna.ir/news/99040100792/ساداتی-نژاد-رییس-کمیسیون-کشاورزی-مجلس-شد \n",
      "3092 (-) -> https://www.isna.ir/news/98100201657/درآمد-۱۴۰-میلیون-یورویی-کشور-از-محل-صادرات-گاز-مایع \n",
      "5190 (-) -> https://www.isna.ir/news/98100201657/درآمد-۱۴۰-میلیون-یورویی-کشور-از-محل-صادرات-گاز-مایع \n",
      "6967 (-) -> https://www.isna.ir/news/98090805130/۹-توصیه-برای-روزهای-آلوده \n",
      "4318 (-) -> https://www.isna.ir/news/98011705394/پاکسازی-آلودگی-نفتی-در-نزدیکی-جزیره-خارگ \n",
      "2156 (-) -> https://www.isna.ir/news/99072115950/فاضلاب-روستاهای-اطراف-سد-وحدت-وارد-شبکه-آب-شهر-می-شود \n",
      "2942 (-) -> https://www.isna.ir/news/98080100533/احمدی-لاشکی-وزیر-نیرو-با-انتقال-آب-خزر-مخالفت-کند-استیضاح-نمی-شود \n",
      "5034 (-) -> https://www.isna.ir/news/98080100533/احمدی-لاشکی-وزیر-نیرو-با-انتقال-آب-خزر-مخالفت-کند-استیضاح-نمی-شود \n",
      "1838 (-) -> https://www.isna.ir/news/99031710007/اعضای-هیات-رییسه-فراکسیون-محیط-زیست-انتخاب-شدند \n",
      "2854 (-) -> https://www.isna.ir/news/98061206732/پاسخ-وزیر-جهاد-کشاورزی-به-سئوالات-اعضای-کمیسیون-کشاورزی \n",
      "3359 (-) -> https://www.isna.ir/news/99022014088/استقرار-فعالان-عرصه-انرژی-در-ساختمان-انرژی-نزدیک-به-صفر \n",
      "3976 (-) -> https://www.isna.ir/news/99091007695/لغو-جلسه-کمیسیون-کشاورزی-مجلس-به-علت-ابتلای-رئیس-سازمان-شیلات \n",
      "5023 (-) -> https://www.isna.ir/news/98072719782/نظر-موافقان-و-مخالفان-درباره-انتقال-آب-دریای-خزر-به-فلات-مرکزی \n",
      "4908 (-) -> https://www.isna.ir/news/98061909326/بررسی-کاربردهای-نانوساختارها-در-صنایع-در-دانشگاه-آزاد-کاشان \n",
      "6315 (-) -> https://www.isna.ir/news/99101208694/آلودگی-هوا-چهارمین-علت-مرگ-زودرس-توصیه-هایی-برای-روزهای-آلوده \n",
      "4446 (-) -> https://www.isna.ir/news/98021608577/صرفه-جویی-۷۰-درصدی-آب-مصرفی-منازل-با-محصولات-دانش-بنیان-حوزه \n",
      "3133 (-) -> https://www.isna.ir/news/98110705292/بررسی-مشکلات-مکانیزاسیون-کشاورزی-در-کمیسیون-مربوطه \n",
      "5288 (-) -> https://www.isna.ir/news/98110705292/بررسی-مشکلات-مکانیزاسیون-کشاورزی-در-کمیسیون-مربوطه \n",
      "5050 (-) -> https://www.isna.ir/news/98081206642/احمدی-لاشکی-احتمال-منتفی-شدن-استیضاح-اردکانیان-وجود-دارد \n",
      "2940 (-) -> https://www.isna.ir/news/98080100603/وکیلی-مجوزی-برای-انتقال-آب-خزر-صادر-نشده-است-انتقال-آب-در-حد \n",
      "5033 (-) -> https://www.isna.ir/news/98080100603/وکیلی-مجوزی-برای-انتقال-آب-خزر-صادر-نشده-است-انتقال-آب-در-حد \n",
      "4891 (-) -> https://www.isna.ir/news/98061407480/ساخت-کارخانه-فروسیلیس-۲۰-هزار-تنی-در-جوکار-همدان \n",
      "3448 (-) -> https://www.isna.ir/news/99031811176/تصویب-ایجاد-ناحیه-صنعتی-جم-و-لزوم-استقرار-صنایع-پاک-و-بدون-آلودگی \n",
      "5151 (-) -> https://www.isna.ir/news/98091712583/اخذ-عوارض-از-کالاهایی-که-پسماندهای-مخرب-دارند \n",
      "2390 (-) -> https://www.isna.ir/news/99102317812/بررسی-وضعیت-محصولات-تراریخته-در-کمیسیون-کشاورزی-مجلس \n",
      "4124 (-) -> https://www.isna.ir/news/99102317812/بررسی-وضعیت-محصولات-تراریخته-در-کمیسیون-کشاورزی-مجلس \n",
      "6184 (-) -> https://www.isna.ir/news/99082114504/۷-باور-اشتباه-درباره-گیاهان-دارویی-و-طب-سنتی-در-همه-گیری-کرونا \n",
      "6187 (-) -> https://www.isna.ir/news/99082114504/۷-باور-اشتباه-درباره-گیاهان-دارویی-و-طب-سنتی-در-همه-گیری-کرونا \n",
      "6972 (-) -> https://www.isna.ir/news/98090905669/بیش-از-۱۲هزار-مراجعه-به-اورژانس-به-دلیل-آلودگی-هوا \n",
      "4731 (-) -> https://www.isna.ir/news/bushehr-584/شرایط-کاری-کارگران-عسلویه-باید-تغییر-کند-عسلویه-۱۰۰-برابر-تهران \n",
      "2993 (-) -> https://www.isna.ir/news/98082717837/تلاش-مسئولان-برای-رفع-نگرانی-مردم-از-آسیب-رسان-بودن-پارازیت-ها \n",
      "6955 (-) -> https://www.isna.ir/news/98082717837/تلاش-مسئولان-برای-رفع-نگرانی-مردم-از-آسیب-رسان-بودن-پارازیت-ها \n",
      "6958 (-) -> https://www.isna.ir/news/98082717837/تلاش-مسئولان-برای-رفع-نگرانی-مردم-از-آسیب-رسان-بودن-پارازیت-ها \n",
      "2199 (-) -> https://www.isna.ir/news/99080502828/وزارت-جهاد-کشاورزی-موظف-به-حذف-انحصار-در-زمینه-شرکت-های-فرآوری \n",
      "3887 (-) -> https://www.isna.ir/news/99080502828/وزارت-جهاد-کشاورزی-موظف-به-حذف-انحصار-در-زمینه-شرکت-های-فرآوری \n",
      "3083 (-) -> https://www.isna.ir/news/98093022449/تقاضای-وکیل-مدافع-مالباختگان-پرونده-رامک-خودرو-از-دادستان-کل \n"
     ]
    }
   ],
   "source": [
    "query_responding(\"normal\", \"heap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_responding(\"normal\", \"other\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
