{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import datetime\n",
    "import collections\n",
    "import math\n",
    "import heapq\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## marks lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_marks = [')','(','>','<',\"؛\",\"،\",'{','}',\"؟\",':',\"-\", '»', '\"', '«', '[', ']','\"','+','=','?']\n",
    "marks = ['/','//', '\\\\','|','!', '%', '&','*','$', '#','؟', '*','.','_' ]\n",
    "alphabet_string_lower = string.ascii_lowercase\n",
    "alphabet_string_upper = string.ascii_uppercase\n",
    "english_char =  list(alphabet_string_lower) + list(alphabet_string_upper)\n",
    "\n",
    "\n",
    "sep_list = [\" \", '\\xad', '\\u200e','\\u200f', '\\u200d', '\\u200d', '\\u200d'] + marks\n",
    "\n",
    "# stop_words1 =['به', 'و', 'در', 'با', 'این', 'شد', 'را', 'که', 'از', 'که', 'این', 'با', 'است', 'برای', 'آن', 'یک', 'خود', 'تا', 'کرد', 'بر', 'هم', 'نیز', 'هزار', 'ریال', 'بود']\n",
    "stop_words = [\"برای\", \"پس\", \"سپس\", \"تا\", \"از\", \"که\", \"و\", \"به\", \"را\", \"با\", \"بر\", \"در\", \"اینکه\", \"این\", \"ان\", \"چرا\",\n",
    "              \"شاید\", \"انها\", \"چون\", \"انطور\", \"اینطور\", \"انچه\", \"آخ\", \"آخر\", \"آخرها\", \"اخه\", \"آره\", \"آری\", \"انان\",\n",
    "              \"اگر\", \"درباره\", \"خیلی\", \"توی\", \"بلکه\", \"بعضی\", \"بعدا\", \"باید\", \"البته\", \"الان\", \"اصلا\", \"اساسا\",\n",
    "              \"احتمالا\", \"اخیرا\", \"ایا\", \"اهان\", \"امرانه\", \"ان گاه\", \"انرا\", \"انقدر\", \"اساسا\", \"بنابراین\", \"هرحال\",\n",
    "              \"بی‌انکه\", \"به‌قدری\", \"بیشتر\", \"کمتر\", \"تر\", \"ترین\", \"تو\", \"چند\", \"چندین\", \"چه\", \"خیلی\", \"دیگر\", \"دیگه\",\n",
    "              \"طبیعتا\", \"عمدا\", \"گاهی\", \"نیز\", \"او\", \"تو\", \"من\", \"ما\", \"طور\", \"دو\", \"هر\", \"همه\", \"ایم\", \"اند\", \"اید\",\n",
    "              \"است\", \"هست\", \"زدگی\", \"خصوصا\", \"ن\", \"ب\", \"کسی\", \"چیزی\", \"بالاخره\", \"وقتی\", \"زمانی\", \"می\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_word_data_20k = ['', 'پیام', 'خواه','دار', 'شده', '۵', 'گزار', 'ها', 'گیر', 'قرار', 'انت', '۴', 'حضور', 'وی', 'می',  'ان', 'بین', 'سال', '۷', 'گفت', 'کار', 'ی', 'کرده', 'اما', 'ادامه', 'شو', 'روز', 'کن', 'گذشته', 'داشت', 'های', 'باز', 'انجام', 'داد', '۳', 'باید', 'اینکه', 'پیش', 'بیش', 'دو', 'باش', 'توان', 'دیگر', '۲', 'عنو',  'امروز', '۹', '۸', 'پای', 'مورد', 'صورت', 'مردم', 'ب', 'هر', 'ما', 'ایر', 'رو', 'اظهار', '۱', 'افزود', '–', '۶', 'ا', 'ر', 'ث',  '۰', 'ش', 'د', 'ژ', 'ک', 'ع', 'ه', 'ج', 'ز', 'م', 'ت', '٢', '٣', '٧', 'ق', 'ل', 'ن', '@', ',', '\\u2066', '٩', '٤', '٥', '٨', '١', '٦', 'س', 'ص', 'ء', 'ف', \"'\", 'ö', 'گ', '》', 'ط', 'پ', '…', 'خ', 'ê', 'ح', '\\u2067', '\\u2069', 'ۗ', '•', '٪', '\\u200b', '×', 'ۚ', 'ۖ', 'چ', '\"', '\\u2063', 'ذ', '\\uf0d8', '۔', 'ü', 'ﻭ', 'غ', '️', 'ﷲ', '\\u202c', '\\u202a', 'ﻫ', 'ۙ', 'ض', '·', '¬', ';', 'خبر گزاری','گزازش','خبرنگار',]\n",
    "# # stop_word_data_17k = ['', 'درصد', 'وی', 'کن', 'توان', 'پیام', 'ما', 'گفت', 'دولت', 'خواه', 'باید', 'اینکه','ان', 'تولید', 'دار', 'اگر', 'هستند', , 'شو', 'پیش', 'وجود','ایر', 'شده', 'کار', 'برخی', 'رو', 'بین', 'توسط', 'گیر', 'نظر', 'باش', 'همچنین', 'سال', 'دو', 'داشت', 'انجام', 'افزای', 'اما', 'هر', 'حال', 'قرار', 'مورد', 'افزود', 'گرفته',  'انها', 'انت', 'دلیل', 'ب', 'ادامه', 'می', 'شرکت', 'اساس', 'پای', 'گزار', 'کرده', 'ساز', 'بخش', 'بیش', 'داد', 'موضوع', 'روز', 'یا', 'شرایط', 'بوده',  'همین', 'کاهش', 'گذشته', 'توجه', 'یکی', 'امروز', 'ماه', 'زار', 'اقدام', '۵', 'داشته', '۲', 'عنو', 'داده', 'های', 'دیگر', 'نسبت', 'بیان', 'همه', '۸', 'اعلام',  'نقل', 'پس', '۷', 'میلیون', '۹', 'ر', '۶', '۴', '۳', 'ه', '۰', 'گروه', 'اشاره', '۱۳۹۹', '۱', 'ج', 'مردم', 'استفاده', 'ی', 'ت', '٥', '٢', '١', '٣', '٦', 'د', '–', 'ژ', 'ص', 'ع', 'م', 'پ', 'ا', '٤', 'ش', '٧', '٩', 'ک', 'ز', 'ف', 'ح', '’', 'ن', ',', '٨', \"'\", 'چ', '٠', '•', '·', '\\u200b', 'غ', '…', 'ل', 'خ', 'ذ', 'ث', '\\uf0d8', 'س', '@', 'ق', 'ء', 'ط', 'گ', ';', '٬', 'ۗ', '\\uf0fc', '—', 'ñ', 'á', 'ﻫ', '×', '−', '±', '\"', '﴾', '°', 'ﻭ',  '🔹', 'ۚ', 'ۖ', '\\u202b', 'ş', 'é', 'ة', 'ۙ']\n",
    "# stop_word_data_11k = ['', 'پیام', 'اینکه', 'ایر', 'شده', 'کن', 'باش', 'کشور', 'های', 'کار', 'روز', 'اعلام', 'گزار', 'همچنین', 'سال', 'کرده', 'داد', 'انت', 'عنو', '۳', 'حضور', 'خواه', 'وی', 'افزود', 'گفت', 'بیان', 'توان', 'شو', 'بخش', 'باید', 'دار', 'داشت', 'ان', 'توجه', 'مورد', 'دو', 'انجام', '۲', 'رو', 'هر', 'یا', 'صورت', 'هستند', 'اما', 'ی', '٢', 'گذشته', '۴', 'قرار', 'بیش', 'دیگر', 'ادامه', 'امروز', '۰', '۶', '۵', '۹', '۷', '۸', '۱', 'ا', '٣', 'ب', 'س', '–', '٨', 'ج', '٧', 'پ', \"'\", 'ز', 'ر', 'ه', 'ع', 'ء', 'م', '\\u202c', 'ش', '٦', 'ن', '٩', '@', 'ۚ', '٤', 'د', 'ص', 'ک', '\\u202b', 'ث', 'چ', 'ل', 'ط', 'ح', 'ف', 'ت', '…', '٥', '\\u2067', '١', 'ذ', 'ق', '—', '٠', ',', 'غ', '\\ufeff', '٪', '\"', '\\u2069', '•', 'ۗ', '’', 'ژ', 'خ', 'и', 'О', 'с', 'б', 'Я', 'С', 'в', '۔', '·','بر','تا','در','به','از', 'گ', 'Ö', 'é', ';', '\\u206b', '》', '⠀', '\\u202a', 'ۖ', '❇', 'ﻫ'] + stop_words1\n",
    "# stop_words = set(stop_word_data_20k + stop_word_data_11k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regex  patterns, prefixe list and postfix list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "raw_postfix = [\n",
    "        ('[\\u200c](تر(ین?)?|گری?|های?)' , ''),\n",
    "        (r'(تر(ین?)?|گری?)(?=$)' , ''), #تر ، ترین، گر، گری در آخر کلمه\n",
    "        (r'(?<=[^ او])(م|ت|ش|مان|تان|شان)$' , ''), # حذف شناسه های مالکیت و فعل در آخر کلمه\n",
    "        (r'(ان|ات|گی|گری|های)$' , ''), #ان، ات، ها، های آخر کلمه  \n",
    "]\n",
    "\n",
    "raw_arabic_notation = [\n",
    "    # remove FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA, KASRA, SHADDA, SUKUN\n",
    "    ('[\\u064B\\u064C\\u064D\\u064E\\u064F\\u0650\\u0651\\u0652]', ''),\n",
    "    ]\n",
    "\n",
    "raw_long_letters = [\n",
    "    (r' +', ' '),  # remove extra spaces\n",
    "    (r'\\n\\n+', '\\n'),  # remove extra newlines\n",
    "    (r'[ـ\\r]', ''),  # remove keshide, carriage returns\n",
    "    ]\n",
    "\n",
    "raw_half_space = [\n",
    "    ('[\\u200c]', ''),\n",
    "]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verbs, mokassar,morakab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_roots =['توان','باش','رو','بر','یاور', 'یانداز', 'یای','یاندیش','بخش','باز','خر','بین','شنو','دار','دان','رسان','شناس','گو','گذار','یاب','لرز','ساز','شو','نویس','خوان','کاه','گیر','خواه','کن' ]\n",
    "\n",
    "past_roots = ['توانست','بود','کرد','اورد','انداخت','امد','خرید','باخت','برد','رفت','اندیشید','بخشید','دید','شنید','داشت','دانست','رساند','شناخت','گفت','گذشت','یافت','لرزید','ساخت','شد','نوشت','خواند','کاست','گرفت','خواست']\n",
    "all_verbs_roots = present_roots + past_roots\n",
    "empty_list = ['','','','','','','']\n",
    "verb_prefix = ['نمی‌', 'می‌','ن','ب',\"\" ]\n",
    "present_verb_postfix = ['م','ی','د','ید','ند','یم']\n",
    "past_verb_postfix = ['ایم','اید','ای','ام','اند']\n",
    "past_verb_postfix2 = ['م','ی','ید','ند','یم']\n",
    "present_verbs = []\n",
    "past_verbs = []\n",
    "all_verbs = {}\n",
    "for pref in verb_prefix:\n",
    "    for present_root, past_root in zip(present_roots, past_roots):\n",
    "        for post in past_verb_postfix2:\n",
    "            all_verbs[pref + past_root+post] = past_root\n",
    "        for post in past_verb_postfix:\n",
    "            all_verbs[past_root+'ه‌'+post] = past_root\n",
    "        for post in present_verb_postfix:\n",
    "            all_verbs[pref+present_root+post] = present_root\n",
    "            \n",
    "words_list = ['ادب', 'آداب', 'طرف', 'اطراف', 'حقیقت', 'حقایق', 'موج', 'امواج', 'ادیب', 'ادبا', 'عمق', 'اعماق', 'خزینه', 'خزائن', 'مرکز', 'مراکز', 'اثر', 'آثار', 'عالم', 'علما', 'خبر', 'اخبار', 'موقع', 'مواقع', 'اسیر', 'اسرا', 'علم', 'علوم', 'دوره', 'ادوار', 'مصرف', 'مصارف', 'اسم', 'اسامی', 'علامت', 'علائم', 'دین', 'ادیان', 'معرفت', 'معارف', 'اسطوره', 'اساطیر', 'علت', 'علل', 'دفتر', 'دفاتر', 'مبحث', 'مباحث', 'امیر', 'امرا', 'عقیده', 'عقائد', 'ذخیره', 'ذخایر', 'ماده', 'مواد', 'امر', 'اوامر', 'عمل', 'اعمال', 'رفیق', 'رفقا', 'مذهب', 'مذاهب', 'امام', 'ائمه', 'عید', 'اعیاد', 'رای', 'آرا', 'مصیبت', 'مصائب', 'اصل', 'اصول', 'عنصر', 'عناصر', 'رسم', 'رسوم', 'معبد', 'معابد', 'افق', 'آفاق', 'عاطفه', 'عواطف', 'رابطه', 'روابط', 'مسجد', 'مساجد', 'بیت', 'ابیات', 'عضو', 'اعضا', 'رمز', 'رموز', 'معبر', 'معابر', 'تاجر', 'تجار', 'عبارت', 'عبارات', 'رجل', 'رجال', 'مظهر', 'مظاهر', 'تصویر', 'تصاویر', 'عجیب', 'عجایب', 'رقم', 'ارقام', 'منظره', 'مناظر', 'جد', 'اجداد', 'فقیه', 'فقها', 'زاویه', 'زوایا', 'مرض', 'امراض', 'جانب', 'جوانب', 'فن', 'فنون', 'زعیم', 'زعما', 'مورد', 'موارد', 'جزیره', 'جزایر', 'فکر', 'افکار', 'سانحه', 'سوانح', 'مرحله', 'مراحل', 'جبل', 'جبال', 'فریضه', 'فرایض', 'سلطان', 'سلاطین', 'مفهوم', 'مفاهیم', 'جریمه', 'جرایم', 'فعل', 'افعال', 'شعر', 'اشعار', 'منبع', 'منابع', 'حادثه', 'حوادث', 'فقیر', 'فقرا', 'شاعر', 'شعرا', 'مکان', 'اماکن', 'حشم', 'احشام', 'قاعده', 'قواعد']\n",
    "mokassar_dict ={}\n",
    "for i in range(0,len(words_list),2):\n",
    "    mokassar_dict[words_list[i+1]] = words_list[i]\n",
    "                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(txt, seps):\n",
    "    default_sep = seps[0]    \n",
    "    # we skip seps[0] because that's the default separator\n",
    "    for sep in seps[1:]:\n",
    "        txt = txt.replace(sep, default_sep)\n",
    "    return [i.strip() for i in txt.split(default_sep)]\n",
    "\n",
    "#get doc and return (word,doc id)\n",
    "def tokenizer(text):\n",
    "    for l in english_char:\n",
    "        text = text.replace(l,\"\")\n",
    "    word_list = split(text.strip().replace(\"\\n\",\" \"), sep_list)\n",
    "    word_list = [x for x in word_list if not x.startswith(\"ir\") and not x.startswith(\"NID\")]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_marks(word_list):\n",
    "#     word_list = list(word_list)\n",
    "    for i in range(len(word_list)):\n",
    "        for mark in punctuation_marks:\n",
    "            word_list[i] = word_list[i].replace(mark,\"\")\n",
    "\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def edit_long_letters(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_long_letters]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_arabic_notation(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_arabic_notation]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def create_translation_table(src_list, dst_list):\n",
    "     return dict((ord(a), b) for a, b in zip(src_list, dst_list)) #map src unicode to dst char\n",
    "\n",
    "    \n",
    "def char_digit_Unification(word_list): \n",
    "    word_list = [x for x in word_list if not x.isnumeric() or (x.isnumeric() and float(x)<3000)]\n",
    "    translation_src, translation_dst = ' ىكي“”', ' یکی\"\"'\n",
    "    translation_src += 'ئ0123456789أإآ%'\n",
    "    translation_dst += 'ی۰۱۲۳۴۵۶۷۸۹ااا٪'\n",
    "    translations_table = create_translation_table(translation_src, translation_dst)\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i].translate(translations_table)\n",
    "    return word_list\n",
    "\n",
    "def remove_mokassar(word_list):\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = mokassar_dict.get(word_list[i],word_list[i])\n",
    "        \n",
    "    return word_list\n",
    "    \n",
    "\n",
    "\n",
    "def verb_Steaming(word_list):\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = all_verbs.get(word_list[i],word_list[i])\n",
    "        \n",
    "    return word_list\n",
    "    \n",
    "\n",
    "def remove_postfix(word_list):\n",
    "\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_postfix]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            if len(word_list[i])>4 and  not (word_list[i] in all_verbs_roots):\n",
    "                word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def remove_prefix(word_list):\n",
    "    starts =['بی‌' , 'نا', 'با']\n",
    "    for i in range(len(word_list)):\n",
    "        for start in starts:\n",
    "            if word_list[i].startswith(start) and len(word_list[i])>4:\n",
    "                word_list[i] = word_list[i][len(start):]\n",
    "                break\n",
    "\n",
    "    return word_list\n",
    "\n",
    "def morakab_Unification(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_half_space]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "#all to gether \n",
    "def normalizer(word_list):\n",
    "    #remove punc\n",
    "    for i in range(len(word_list)):\n",
    "        for mark in punctuation_marks:\n",
    "            word_list[i] = word_list[i].replace(mark,\"\")\n",
    "            \n",
    "    #edit long letters\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_long_letters]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "            \n",
    "    #remove mokassar\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = mokassar_dict.get(word_list[i],word_list[i])\n",
    "    \n",
    "    #remove_arabic_notation\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_arabic_notation]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    \n",
    "    #char_digit_Unification\n",
    "    word_list = [x for x in word_list if not x.isnumeric() or (x.isnumeric() and float(x)<3000)]\n",
    "    translation_src, translation_dst = ' ىكي“”', ' یکی\"\"'\n",
    "    translation_src += 'ئ0123456789أإآ%'\n",
    "    translation_dst += 'ی۰۱۲۳۴۵۶۷۸۹ااا٪'\n",
    "    translations_table = create_translation_table(translation_src, translation_dst)\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i].translate(translations_table)\n",
    "    \n",
    "    #verb_Steaming\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = all_verbs.get(word_list[i],word_list[i])\n",
    "        \n",
    "    #remove_prefix\n",
    "    starts =['بی‌' , 'نا', 'با']\n",
    "    for i in range(len(word_list)):\n",
    "        for start in starts:\n",
    "            if word_list[i].startswith(start) and len(word_list[i])>4:\n",
    "                word_list[i] = word_list[i][len(start):]\n",
    "                break\n",
    "    \n",
    "    #remove_postfix\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_postfix]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            if len(word_list[i])>4 and  not (word_list[i] in all_verbs_roots):\n",
    "                word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    #morakab_Unification\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_half_space]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "            \n",
    "    for word in word_list:\n",
    "        if word in stop_words:\n",
    "            word_list.remove(word)\n",
    "    \n",
    "    return [x for x in word_list if len(x) >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_data = pd.DataFrame()\n",
    "pd_list = []\n",
    "pd_list .append(pd.read_excel('IR00_3_11k News.xlsx', engine = 'openpyxl'))\n",
    "pd_list .append(pd.read_excel('IR00_3_17k News.xlsx', engine = 'openpyxl'))\n",
    "pd_list .append(pd.read_excel('IR00_3_20k News.xlsx', engine = 'openpyxl'))\n",
    "# pd_list .append(pd.read_excel('3.xlsx', engine = 'openpyxl'))\n",
    "# pd_list .append(pd.read_excel('2.xlsx', engine = 'openpyxl'))\n",
    "# pd_list .append(pd.read_excel('1.xlsx', engine = 'openpyxl'))\n",
    "for df in pd_list:\n",
    "    all_data = all_data.append(df,ignore_index=True)\n",
    "all_data[\"topic\"].replace({\"political\": \"politics\", \"sport\": \"sports\"}, inplace=True)\n",
    "all_data = all_data[all_data.content != 'None\\n\\n']\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "all_data.index +=1\n",
    "all_data['id'] = all_data.index\n",
    "contents = all_data['content'].to_list()\n",
    "doc_ids = all_data['id'].to_list()\n",
    "doc_url = all_data['url']\n",
    "docs_topic = all_data['topic']\n",
    "docs_num = len(doc_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create inverted index and change docs content to bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dictionary(contents, doc_ids):\n",
    "    for content, id in zip(contents,doc_ids):\n",
    "        doc_tokens = tokenizer(content)\n",
    "        doc_normalized_tokens = normalizer(doc_tokens)\n",
    "        term_freq_dict = dict(collections.Counter(doc_normalized_tokens))\n",
    "        \n",
    "        for term, freq in term_freq_dict.items():\n",
    "            term_freq_dict[term] = (1 + math.log((freq), 10))\n",
    "        contents[id-1] = term_freq_dict\n",
    "        for term,freq in term_freq_dict.items():\n",
    "            term_postings = inverted_index.get(term,{})\n",
    "            term_postings[id] = freq\n",
    "            inverted_index[term] = term_postings\n",
    "\n",
    "def calculate_tf_idf_weight(contents):\n",
    "    docs_len = []\n",
    "    for doc in contents:\n",
    "        length = 0\n",
    "        for term in doc.keys():\n",
    "            df = len(inverted_index[term])\n",
    "            idf = math.log(docs_num/df,10)\n",
    "            doc[term] *= idf \n",
    "            length += doc[term]\n",
    "        docs_len.append( math.sqrt(length))\n",
    "    return docs_len\n",
    "\n",
    "            \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inverted_index ={}\n",
    "create_dictionary(contents, doc_ids)\n",
    "docs_len = calculate_tf_idf_weight(contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save inverted_index and docs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_inverted_index = open('ph3_inverted_index.obj', 'wb') \n",
    "pickle.dump(inverted_index, file_inverted_index)\n",
    "file_inverted_index.close()\n",
    "inverted_index = {}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_docs_len = open('ph3_docs_len.obj', 'wb') \n",
    "pickle.dump(docs_len, file_docs_len)\n",
    "file_docs_len.close()\n",
    "docs_len = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = open('ph3_contents.obj', 'wb') \n",
    "pickle.dump(contents, file_contents)\n",
    "file_contents.close()\n",
    "contents =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inverted_index['سلام']\n",
    "# docs_len =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read inverted_index and docs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "readed = open('ph3_inverted_index.obj', 'rb') \n",
    "inverted_index = pickle.load(readed)\n",
    "readed = open('docs_len.obj', 'rb') \n",
    "docs_len = pickle.load(readed)\n",
    "readed = open('ph3_contents.obj', 'rb') \n",
    "contents = pickle.load(readed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans Clustring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_centroids(k):\n",
    "    centroid_docs_id = (np.random.uniform(0, docs_num, size = k).astype(int))\n",
    "    centroids = { x:contents[x-1] for x in centroid_docs_id}\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_to_leader(centeroids,centroids_len, contents, first):\n",
    "    \n",
    "    clusters = {x:[] for x in centeroids.keys()}\n",
    "    for doc_id in doc_ids:\n",
    "        relevent_flag = False\n",
    "        doc_content = contents[doc_id-1]\n",
    "        max_Score = 0\n",
    "        best_centroid = 0\n",
    "        for centroid_id, centroid_content in centeroids.items():\n",
    "            centroid_score = 0\n",
    "            for term in doc_content.keys():\n",
    "                centroid_score += (doc_content[term] * centroid_content.get(term, 0))\n",
    "            \n",
    "            if first == 0:\n",
    "                centroid_score /= docs_len[centroid_id-1]\n",
    "            else: \n",
    "                centroid_score /= centroids_len[centroid_id]\n",
    "            \n",
    "            if centroid_score> max_Score:\n",
    "                max_Score = centroid_score\n",
    "                best_centroid = centroid_id\n",
    "                relevent_flag = True\n",
    "        if relevent_flag:\n",
    "            clusters[best_centroid].append(doc_id)\n",
    "    return clusters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_cluster(clusters, k):\n",
    "    new_centroids = {-1*x:{} for x in range(1,k+1)}\n",
    "    centroids_len = {-1*x:0 for x in range(1,k+1)}\n",
    "    counter = -1\n",
    "    for cluster_followers in clusters.values():\n",
    "        new_centroid = {}\n",
    "        cluster_sum = 0\n",
    "        for doc_id in cluster_followers:\n",
    "            for term,w_t in contents[doc_id-1].items():\n",
    "                w = new_centroid.get(term,0)\n",
    "                new_centroid[term] = w + w_t\n",
    "                \n",
    "        for key in new_centroid.keys():\n",
    "            new_centroid[key] /= len(cluster_followers)\n",
    "            cluster_sum += new_centroid[key]**2\n",
    "        new_centroids[counter] = new_centroid\n",
    "        centroids_len[counter] =math.sqrt(cluster_sum)\n",
    "        counter -= 1\n",
    "    return  new_centroids, centroids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmeans_clustering(k, number_of_iteration):\n",
    "    new_centroids = choose_random_centroids(k)\n",
    "    clusters = []\n",
    "    centroids_len = {}\n",
    "    for i in tqdm(range(number_of_iteration)):\n",
    "        clusters =  assign_to_leader(new_centroids,centroids_len ,contents, i)\n",
    "\n",
    "        if i != number_of_iteration-1: \n",
    "            new_centroids, centroids_len = avg_cluster(clusters, k)\n",
    "\n",
    "    return clusters, new_centroids, centroids_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# clusters, centroids, centroids_len = Kmeans_clustering(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [19:52<00:00, 238.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clusters, centroids, centroids_len = Kmeans_clustering(100,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673\n",
      "765\n",
      "662\n",
      "564\n",
      "580\n",
      "90\n",
      "869\n",
      "416\n",
      "141\n",
      "1254\n",
      "506\n",
      "576\n",
      "187\n",
      "262\n",
      "819\n",
      "240\n",
      "1031\n",
      "462\n",
      "241\n",
      "484\n",
      "598\n",
      "157\n",
      "1156\n",
      "482\n",
      "258\n",
      "308\n",
      "247\n",
      "384\n",
      "853\n",
      "445\n",
      "136\n",
      "617\n",
      "645\n",
      "322\n",
      "304\n",
      "565\n",
      "824\n",
      "364\n",
      "528\n",
      "745\n",
      "680\n",
      "150\n",
      "482\n",
      "613\n",
      "869\n",
      "1194\n",
      "146\n",
      "780\n",
      "500\n",
      "391\n",
      "78\n",
      "138\n",
      "787\n",
      "526\n",
      "445\n",
      "601\n",
      "529\n",
      "406\n",
      "357\n",
      "795\n",
      "181\n",
      "784\n",
      "163\n",
      "264\n",
      "169\n",
      "693\n",
      "590\n",
      "597\n",
      "242\n",
      "717\n",
      "397\n",
      "271\n",
      "300\n",
      "291\n",
      "92\n",
      "519\n",
      "199\n",
      "230\n",
      "152\n",
      "543\n",
      "783\n",
      "279\n",
      "591\n",
      "567\n",
      "616\n",
      "504\n",
      "547\n",
      "504\n",
      "629\n",
      "356\n",
      "731\n",
      "586\n",
      "1047\n",
      "354\n",
      "485\n",
      "455\n",
      "585\n",
      "439\n",
      "521\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "for key in clusters.keys():\n",
    "    print(len(clusters[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save clusters data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_clusters = open('clusters.obj', 'wb') \n",
    "pickle.dump(clusters, file_clusters)\n",
    "file_clusters.close()\n",
    "clusters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_centroids = open('centroids.obj', 'wb') \n",
    "pickle.dump(centroids, file_centroids)\n",
    "file_centroids.close()\n",
    "centroids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_centroids_len = open('centroids_len.obj', 'wb') \n",
    "pickle.dump(centroids_len, file_centroids_len)\n",
    "file_centroids_len.close()\n",
    "centroids_len = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read clusters data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "readed = open('clusters.obj', 'rb') \n",
    "clusters = pickle.load(readed)\n",
    "readed = open('centroids.obj', 'rb') \n",
    "centroids = pickle.load(readed)\n",
    "readed = open('centroids_len.obj', 'rb') \n",
    "centroids_len = pickle.load(readed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query responding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_query(query_str):\n",
    "    query_tokens = tokenizer(query_str)\n",
    "    query_normalized_tokens = normalizer(query_tokens)\n",
    "    term_freq_dict = dict(collections.Counter(query_normalized_tokens))\n",
    "    query_terms = {}\n",
    "    for  term,freq in term_freq_dict.items():\n",
    "        if term in inverted_index.keys() and term not in stop_words:\n",
    "            query_terms[term] = 1+math.log(freq,10)\n",
    "    print(query_terms)\n",
    "    return query_terms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_leader(centroids, centroids_len, query_terms, k,b):\n",
    "    similarity_score = {-1*x:0 for x in range(1,k+1)}\n",
    "    for term, w_q in query_terms.items():\n",
    "        for centroid_id, centroid_content in centroids.items():\n",
    "            similarity_score[centroid_id] += w_q * centroid_content.get(term, 0)\n",
    "    for key in similarity_score.keys():\n",
    "        similarity_score[key] /= centroids_len[key]\n",
    "    return sorted(similarity_score, key=similarity_score.get, reverse=True)[:b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_docs(query_terms, cluster_docs):\n",
    "    docs_score = {x:0 for x in cluster_docs}\n",
    "    for term,Wt_q in query_terms.items():\n",
    "        for doc_id in cluster_docs:\n",
    "            docs_score[doc_id] += Wt_q * contents[doc_id-1].get(term, 0)\n",
    "    \n",
    "    for doc_id in cluster_docs:\n",
    "        docs_score[doc_id] /= docs_len[doc_id-1]\n",
    "    return docs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_k(docs_similarity, k):\n",
    "\n",
    "    heap = [(-value, key) for key,value in docs_similarity.items()]\n",
    "    largest = heapq.nsmallest(k, heap)\n",
    "    top_k = [(y, -x) for x,y in largest]\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_responding(clusters_num, b):\n",
    "    query = input(\"enter query: \")\n",
    "    query_terms = vectorize_query(query)\n",
    "    best_leader_ids = find_best_leader(centroids, centroids_len, query_terms, clusters_num,b)\n",
    "    print(best_leader_ids)\n",
    "    cluster_docs = list(set(clusters[best_leader_ids[0]] + clusters[best_leader_ids[1]]))\n",
    "    docs_similarity = score_docs(query_terms, cluster_docs)\n",
    "    a = datetime.datetime.now()\n",
    "    top_docs = extract_top_k(docs_similarity, 50)\n",
    "    b = datetime.datetime.now()\n",
    "\n",
    "    print(\"{:<5} result in {} ms\\nid \\t(score)\\t\\t  -> link\\n\".format(len(top_docs),1000*(b-a).total_seconds()))\n",
    "    for doc_id, score in top_docs:\n",
    "        print( \"{:<5}({}) -> {} \".format(doc_id,\"-\", doc_url[doc_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter query: اعزام کاروان های اردوی راهیان نور\n",
      "{'اعزام': 1.0, 'کارو': 1.0, 'های': 1.0, 'اردوی': 1.0, 'راهی': 1.0, 'نور': 1.0}\n",
      "[-32, -41, -15, -33]\n",
      "50    result in 0.0 ms\n",
      "id \t(score)\t\t  -> link\n",
      "\n",
      "9702 (-) -> https://www.isna.ir/news/98021106170/حضور-لرستانی-ها-در-آیین-افتتاحیه-اردوهای-راهیان-نور-غرب-کشور \n",
      "9934 (-) -> https://www.isna.ir/news/98032712899/حضور-دانشجویان-چهارمحال-و-بختیاری-در-یادمان-های-دفاع-مقدس \n",
      "9004 (-) -> https://www.isna.ir/news/99092821569/سردار-کارگر-ماهیت-راهیان-نور-نباید-تغییر-کند \n",
      "10656(-) -> https://www.isna.ir/news/98080904715/مراسم-افتتاحیه-راهیان-نور-دانش-آموزی-برگزار-می-شود \n",
      "10031(-) -> https://www.isna.ir/news/98041910370/خوزستانی-ها-به-مناطق-عملیاتی-غرب-کشور-می-روند \n",
      "10043(-) -> https://www.isna.ir/news/98041910370/خوزستانی-ها-به-مناطق-عملیاتی-غرب-کشور-می-روند \n",
      "10692(-) -> https://www.isna.ir/news/98081408553/اعلام-جزئیات-حضور-دانش-آموزان-گیلانی-در-اردوهای-راهیان-نور \n",
      "10835(-) -> https://www.isna.ir/news/98091309743/توضیحات-سردار-عبدالله-پور-درباره-بزرگترین-کاروان-راهیان-نور-کشور \n",
      "10379(-) -> https://www.isna.ir/news/98062110107/میزبانی-از-۱۰۰-هزار-زائر-راهیان-نور-در-اردوگاه-شهید-رستگار \n",
      "10318(-) -> https://www.isna.ir/news/98061005201/کارگاه-توانمندی-سازی-مدیران-کاروان-های-راهیان-نور-برگزار-شد \n",
      "10025(-) -> https://www.isna.ir/news/98041708948/۲۵-هزار-دانش-آموز-کردستانی-به-راهیان-نور-می-روند \n",
      "8518 (-) -> https://www.isna.ir/news/99070806230/۱۵-یادمان-دفاع-مقدس-به-عنوان-آثار-ملی-دفاع-مقدس-ثبت-شده-است \n",
      "10121(-) -> https://www.isna.ir/news/98050502469/سفر-بیش-از-۱۲-هزار-نفر-به-اردوهای-فرهنگی-توسط-سپاه-قزوین \n",
      "9588 (-) -> https://www.isna.ir/news/98012208823/رشد-۳۰-درصدی-اعزام-زائران-راهیان-نور-در-کرمان \n",
      "11170(-) -> https://www.isna.ir/news/98111511016/حضور-۲۵-هزار-نفر-از-مازندران-در-یادمان-های-دفاع-مقدس \n",
      "9312 (-) -> https://www.isna.ir/news/99112619254/اردوی-راهیان-نور-مجازی-دانش-آموزان-اردبیل-برگزار-می-شود \n",
      "9514 (-) -> https://www.isna.ir/news/98010501105/مناطق-عملیاتی-خوزستان-هنوز-آلوده-به-مین-هستند \n",
      "11065(-) -> https://www.isna.ir/news/98102419036/رشد-۲۷-درصدی-اردوهای-راهیان-نور-دانش-آموزی-استان-کرمانشاه \n",
      "10961(-) -> https://www.isna.ir/news/98100604120/گزارشی-از-حضور-گیلانی-ها-در-مناطق-عملیاتی \n",
      "11415(-) -> https://www.isna.ir/news/98122720736/شهادت-سه-خادم-راهیان-نور-در-راه-مبارزه-با-کرونا \n",
      "10223(-) -> https://www.isna.ir/news/98052311624/تشریح-حضور-کاروان-های-راهیان-نور-در-یادمان-های-دفاع-مقدس \n",
      "10232(-) -> https://www.isna.ir/news/98052311624/تشریح-حضور-کاروان-های-راهیان-نور-در-یادمان-های-دفاع-مقدس \n",
      "10090(-) -> https://www.isna.ir/news/98043015977/تشریح-جزئیات-حضور-مردم-اردبیل-در-یادمان-های-شمال-غرب-کشور \n",
      "10800(-) -> https://www.isna.ir/news/98090604281/آماری-از-حضور-زائران-راهیان-نور-غرب-کشور \n",
      "9258 (-) -> https://www.isna.ir/news/99111410631/اهدای-نشان-ملی-خادمی-شهدا-به-سردار-مطهری \n",
      "10510(-) -> https://www.isna.ir/news/98071310002/حضور-نخستین-کاروان-راهیان-نور-دانش-آموزی-اردبیل-در-یادمان-های \n",
      "8530 (-) -> https://www.isna.ir/news/99071007594/وزارت-تعاون-مسئولیت-سه-یادمان-دفاع-مقدس-را-دارد \n",
      "9712 (-) -> https://www.isna.ir/news/98021407389/سردار-سلیمانی-نقش-مردم-در-راهیان-نور-برجسته-بود \n",
      "7863 (-) -> https://www.isna.ir/news/99040100559/اعزام-کاروان-های-راهیان-نور-فعلا-منتفی-شده-است \n",
      "10723(-) -> https://www.isna.ir/news/98082113787/گزارشی-از-وضعیت-بازدید-زائران-راهیان-نور-کرمانشاه \n",
      "30800(-) -> https://www.farsnews.ir/news/14000123000337/اعزام-کمانداران-به-کاپ-جهانی-لوزان \n",
      "10877(-) -> https://www.isna.ir/news/98092015284/گرامیداشت-عملیات-مطلع-الفجر-در-ارتفاعات-گیلانفرب \n",
      "10569(-) -> https://www.isna.ir/news/98072418585/نقش-خادمین-شهدا-در-برپایی-موکب-های-پذیرایی-از-زوار-حسینی \n",
      "10859(-) -> https://www.isna.ir/news/98091712620/سایه-کاهش-بودجه-بر-سر-ترویج-فرهنگ-دفاع-مقدس \n",
      "10420(-) -> https://www.isna.ir/news/98062612611/برگزاری-اردویی-در-راستای-جریان-شناسی-منافقین-در-مازندران \n",
      "9512 (-) -> https://www.isna.ir/news/98010501130/اسکان-۲۰۰۰-زائر-راهیان-نور-در-اردوگاه-های-آبادان \n",
      "32583(-) -> https://www.farsnews.ir/news/13991229000552/اسامی-جودوکاران-تیم-ملی-اعزامی-به-قهرمانی-آسیا-اعلام-شد \n",
      "9395 (-) -> https://www.isna.ir/news/99121107995/استفاده-حداکثری-از-ظرفیت-فضای-مجازی-برای-برگزاری-اردوهای-راهیان \n",
      "9406 (-) -> https://www.isna.ir/news/99121410862/حضور-بیشتر-از-۲۰-هزار-دانش-آموز-گلستانی-در-راهیان-نور-مجازی \n",
      "11082(-) -> https://www.isna.ir/news/98102821723/حضور-۵۰-هزار-نفری-مردم-آذربایجان-غربی-در-یادمان-های-دفاع-مقدس \n",
      "11163(-) -> https://www.isna.ir/news/98111409909/آشنا-وظیفه-دولت-پشتیبانی-از-حرکت-فرهنگی-و-مردمی-راهیان-نور-است \n",
      "11352(-) -> https://www.isna.ir/news/98121713243/فعالیت-گروه-های-جهادی-سمنان-برای-مقابله-با-ویروس-کرونا \n",
      "29085(-) -> https://www.farsnews.ir/news/14000207000678/تیم-ملی-بوکس-راهی-ازبکستان-می‌شود \n",
      "9341 (-) -> https://www.isna.ir/news/99120201018/حضور-دانش-آموزان-در-راهیان-نور-مجازی \n",
      "38782(-) -> https://www.farsnews.ir/news/13991107000374/یزدانی-خیلی-دوست-داشتم-در-صربستان-شرکت-کنم-شرایطم-پس-از-کرونا-خوب \n",
      "8925 (-) -> https://www.isna.ir/news/99091310235/اهدای-لوح-و-نشان-خادمی-شهدا-به-فرمانده-مرزبانی-نیروی-انتظامی \n",
      "38662(-) -> https://www.farsnews.ir/news/13991108000262/اردوی-تیم-ملی-والیبال-ساحلی-تعطیل-شد-کرونا-کار-دست-ساحلی‌بازان-داد \n",
      "10127(-) -> https://www.isna.ir/news/98050603092/حضور-اقشار-مختلف-مردم-در-یادمان-های-دفاع-مقدس-آذربایجان-غربی \n",
      "34298(-) -> https://www.farsnews.ir/news/13991214000401/تمرین-مشترک-تیم‌های-بوکس-جوانان-ایران-و-آذربایجان-فیلم \n",
      "33711(-) -> https://www.farsnews.ir/news/13991219000118/آغاز-اردوی-تیم-ملی-جودو-پس-از-پایان-سوپر-لیگ \n"
     ]
    }
   ],
   "source": [
    "query_responding(100,4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
