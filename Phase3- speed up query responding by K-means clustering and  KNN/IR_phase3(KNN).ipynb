{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import datetime\n",
    "import collections\n",
    "import math\n",
    "import heapq\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## marks lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_marks = [')','(','>','<',\"؛\",\"،\",'{','}',\"؟\",':',\"-\", '»', '\"', '«', '[', ']','\"','+','=','?']\n",
    "marks = ['/','//', '\\\\','|','!', '%', '&','*','$', '#','؟', '*','.','_' ]\n",
    "alphabet_string_lower = string.ascii_lowercase\n",
    "alphabet_string_upper = string.ascii_uppercase\n",
    "english_char =  list(alphabet_string_lower) + list(alphabet_string_upper)\n",
    "\n",
    "\n",
    "sep_list = [\" \", '\\xad', '\\u200e','\\u200f', '\\u200d', '\\u200d', '\\u200d'] + marks\n",
    "\n",
    "# stop_words1 =['به', 'و', 'در', 'با', 'این', 'شد', 'را', 'که', 'از', 'که', 'این', 'با', 'است', 'برای', 'آن', 'یک', 'خود', 'تا', 'کرد', 'بر', 'هم', 'نیز', 'هزار', 'ریال', 'بود']\n",
    "stop_words = [\"برای\", \"پس\", \"سپس\", \"تا\", \"از\", \"که\", \"و\", \"به\", \"را\", \"با\", \"بر\", \"در\", \"اینکه\", \"این\", \"ان\", \"چرا\",\n",
    "              \"شاید\", \"انها\", \"چون\", \"انطور\", \"اینطور\", \"انچه\", \"آخ\", \"آخر\", \"آخرها\", \"اخه\", \"آره\", \"آری\", \"انان\",\n",
    "              \"اگر\", \"درباره\", \"خیلی\", \"توی\", \"بلکه\", \"بعضی\", \"بعدا\", \"باید\", \"البته\", \"الان\", \"اصلا\", \"اساسا\",\n",
    "              \"احتمالا\", \"اخیرا\", \"ایا\", \"اهان\", \"امرانه\", \"ان گاه\", \"انرا\", \"انقدر\", \"اساسا\", \"بنابراین\", \"هرحال\",\n",
    "              \"بی‌انکه\", \"به‌قدری\", \"بیشتر\", \"کمتر\", \"تر\", \"ترین\", \"تو\", \"چند\", \"چندین\", \"چه\", \"خیلی\", \"دیگر\", \"دیگه\",\n",
    "              \"طبیعتا\", \"عمدا\", \"گاهی\", \"نیز\", \"او\", \"تو\", \"من\", \"ما\", \"طور\", \"دو\", \"هر\", \"همه\", \"ایم\", \"اند\", \"اید\",\n",
    "              \"است\", \"هست\", \"زدگی\", \"خصوصا\", \"ن\", \"ب\", \"کسی\", \"چیزی\", \"بالاخره\", \"وقتی\", \"زمانی\", \"می\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_word_data_20k = ['', 'پیام', 'خواه','دار', 'شده', '۵', 'گزار', 'ها', 'گیر', 'قرار', 'انت', '۴', 'حضور', 'وی', 'می',  'ان', 'بین', 'سال', '۷', 'گفت', 'کار', 'ی', 'کرده', 'اما', 'ادامه', 'شو', 'روز', 'کن', 'گذشته', 'داشت', 'های', 'باز', 'انجام', 'داد', '۳', 'باید', 'اینکه', 'پیش', 'بیش', 'دو', 'باش', 'توان', 'دیگر', '۲', 'عنو',  'امروز', '۹', '۸', 'پای', 'مورد', 'صورت', 'مردم', 'ب', 'هر', 'ما', 'ایر', 'رو', 'اظهار', '۱', 'افزود', '–', '۶', 'ا', 'ر', 'ث',  '۰', 'ش', 'د', 'ژ', 'ک', 'ع', 'ه', 'ج', 'ز', 'م', 'ت', '٢', '٣', '٧', 'ق', 'ل', 'ن', '@', ',', '\\u2066', '٩', '٤', '٥', '٨', '١', '٦', 'س', 'ص', 'ء', 'ف', \"'\", 'ö', 'گ', '》', 'ط', 'پ', '…', 'خ', 'ê', 'ح', '\\u2067', '\\u2069', 'ۗ', '•', '٪', '\\u200b', '×', 'ۚ', 'ۖ', 'چ', '\"', '\\u2063', 'ذ', '\\uf0d8', '۔', 'ü', 'ﻭ', 'غ', '️', 'ﷲ', '\\u202c', '\\u202a', 'ﻫ', 'ۙ', 'ض', '·', '¬', ';', 'خبر گزاری','گزازش','خبرنگار',]\n",
    "# # stop_word_data_17k = ['', 'درصد', 'وی', 'کن', 'توان', 'پیام', 'ما', 'گفت', 'دولت', 'خواه', 'باید', 'اینکه','ان', 'تولید', 'دار', 'اگر', 'هستند', , 'شو', 'پیش', 'وجود','ایر', 'شده', 'کار', 'برخی', 'رو', 'بین', 'توسط', 'گیر', 'نظر', 'باش', 'همچنین', 'سال', 'دو', 'داشت', 'انجام', 'افزای', 'اما', 'هر', 'حال', 'قرار', 'مورد', 'افزود', 'گرفته',  'انها', 'انت', 'دلیل', 'ب', 'ادامه', 'می', 'شرکت', 'اساس', 'پای', 'گزار', 'کرده', 'ساز', 'بخش', 'بیش', 'داد', 'موضوع', 'روز', 'یا', 'شرایط', 'بوده',  'همین', 'کاهش', 'گذشته', 'توجه', 'یکی', 'امروز', 'ماه', 'زار', 'اقدام', '۵', 'داشته', '۲', 'عنو', 'داده', 'های', 'دیگر', 'نسبت', 'بیان', 'همه', '۸', 'اعلام',  'نقل', 'پس', '۷', 'میلیون', '۹', 'ر', '۶', '۴', '۳', 'ه', '۰', 'گروه', 'اشاره', '۱۳۹۹', '۱', 'ج', 'مردم', 'استفاده', 'ی', 'ت', '٥', '٢', '١', '٣', '٦', 'د', '–', 'ژ', 'ص', 'ع', 'م', 'پ', 'ا', '٤', 'ش', '٧', '٩', 'ک', 'ز', 'ف', 'ح', '’', 'ن', ',', '٨', \"'\", 'چ', '٠', '•', '·', '\\u200b', 'غ', '…', 'ل', 'خ', 'ذ', 'ث', '\\uf0d8', 'س', '@', 'ق', 'ء', 'ط', 'گ', ';', '٬', 'ۗ', '\\uf0fc', '—', 'ñ', 'á', 'ﻫ', '×', '−', '±', '\"', '﴾', '°', 'ﻭ',  '🔹', 'ۚ', 'ۖ', '\\u202b', 'ş', 'é', 'ة', 'ۙ']\n",
    "# stop_word_data_11k = ['', 'پیام', 'اینکه', 'ایر', 'شده', 'کن', 'باش', 'کشور', 'های', 'کار', 'روز', 'اعلام', 'گزار', 'همچنین', 'سال', 'کرده', 'داد', 'انت', 'عنو', '۳', 'حضور', 'خواه', 'وی', 'افزود', 'گفت', 'بیان', 'توان', 'شو', 'بخش', 'باید', 'دار', 'داشت', 'ان', 'توجه', 'مورد', 'دو', 'انجام', '۲', 'رو', 'هر', 'یا', 'صورت', 'هستند', 'اما', 'ی', '٢', 'گذشته', '۴', 'قرار', 'بیش', 'دیگر', 'ادامه', 'امروز', '۰', '۶', '۵', '۹', '۷', '۸', '۱', 'ا', '٣', 'ب', 'س', '–', '٨', 'ج', '٧', 'پ', \"'\", 'ز', 'ر', 'ه', 'ع', 'ء', 'م', '\\u202c', 'ش', '٦', 'ن', '٩', '@', 'ۚ', '٤', 'د', 'ص', 'ک', '\\u202b', 'ث', 'چ', 'ل', 'ط', 'ح', 'ف', 'ت', '…', '٥', '\\u2067', '١', 'ذ', 'ق', '—', '٠', ',', 'غ', '\\ufeff', '٪', '\"', '\\u2069', '•', 'ۗ', '’', 'ژ', 'خ', 'и', 'О', 'с', 'б', 'Я', 'С', 'в', '۔', '·','بر','تا','در','به','از', 'گ', 'Ö', 'é', ';', '\\u206b', '》', '⠀', '\\u202a', 'ۖ', '❇', 'ﻫ'] + stop_words1\n",
    "# stop_words = set(stop_word_data_20k + stop_word_data_11k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regex  patterns, prefixe list and postfix list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "raw_postfix = [\n",
    "        ('[\\u200c](تر(ین?)?|گری?|های?)' , ''),\n",
    "        (r'(تر(ین?)?|گری?)(?=$)' , ''), #تر ، ترین، گر، گری در آخر کلمه\n",
    "       # (r'(?<=[^ او])(م|ت|ش|مان|تان|شان|ی)$' , ''), # حذف شناسه های مالکیت و فعل در آخر کلمه\n",
    "        (r'(?<=[^ او])(م|ت|ش|مان|تان|شان)$' , ''), # حذف شناسه های مالکیت و فعل در آخر کلمه\n",
    "        (r'(ان|ات|گی|گری|های)$' , ''), #ان، ات، ها، های آخر کلمه  \n",
    "]\n",
    "\n",
    "raw_arabic_notation = [\n",
    "    # remove FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA, KASRA, SHADDA, SUKUN\n",
    "    ('[\\u064B\\u064C\\u064D\\u064E\\u064F\\u0650\\u0651\\u0652]', ''),\n",
    "    ]\n",
    "\n",
    "raw_long_letters = [\n",
    "    (r' +', ' '),  # remove extra spaces\n",
    "    (r'\\n\\n+', '\\n'),  # remove extra newlines\n",
    "    (r'[ـ\\r]', ''),  # remove keshide, carriage returns\n",
    "    ]\n",
    "\n",
    "raw_half_space = [\n",
    "    ('[\\u200c]', ''),\n",
    "]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verbs, mokassar,morakab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_roots =['توان','باش','رو','بر','یاور', 'یانداز', 'یای','یاندیش','بخش','باز','خر','بین','شنو','دار','دان','رسان','شناس','گو','گذار','یاب','لرز','ساز','شو','نویس','خوان','کاه','گیر','خواه','کن' ]\n",
    "\n",
    "past_roots = ['توانست','بود','کرد','اورد','انداخت','امد','خرید','باخت','برد','رفت','اندیشید','بخشید','دید','شنید','داشت','دانست','رساند','شناخت','گفت','گذشت','یافت','لرزید','ساخت','شد','نوشت','خواند','کاست','گرفت','خواست']\n",
    "all_verbs_roots = present_roots + past_roots\n",
    "empty_list = ['','','','','','','']\n",
    "verb_prefix = ['نمی‌', 'می‌','ن','ب',\"\" ]\n",
    "present_verb_postfix = ['م','ی','د','ید','ند','یم']\n",
    "past_verb_postfix = ['ایم','اید','ای','ام','اند']\n",
    "past_verb_postfix2 = ['م','ی','ید','ند','یم']\n",
    "present_verbs = []\n",
    "past_verbs = []\n",
    "all_verbs = {}\n",
    "for pref in verb_prefix:\n",
    "    for present_root, past_root in zip(present_roots, past_roots):\n",
    "        for post in past_verb_postfix2:\n",
    "            all_verbs[pref + past_root+post] = past_root\n",
    "        for post in past_verb_postfix:\n",
    "            all_verbs[past_root+'ه‌'+post] = past_root\n",
    "        for post in present_verb_postfix:\n",
    "            all_verbs[pref+present_root+post] = present_root\n",
    "            \n",
    "words_list = ['ادب', 'آداب', 'طرف', 'اطراف', 'حقیقت', 'حقایق', 'موج', 'امواج', 'ادیب', 'ادبا', 'عمق', 'اعماق', 'خزینه', 'خزائن', 'مرکز', 'مراکز', 'اثر', 'آثار', 'عالم', 'علما', 'خبر', 'اخبار', 'موقع', 'مواقع', 'اسیر', 'اسرا', 'علم', 'علوم', 'دوره', 'ادوار', 'مصرف', 'مصارف', 'اسم', 'اسامی', 'علامت', 'علائم', 'دین', 'ادیان', 'معرفت', 'معارف', 'اسطوره', 'اساطیر', 'علت', 'علل', 'دفتر', 'دفاتر', 'مبحث', 'مباحث', 'امیر', 'امرا', 'عقیده', 'عقائد', 'ذخیره', 'ذخایر', 'ماده', 'مواد', 'امر', 'اوامر', 'عمل', 'اعمال', 'رفیق', 'رفقا', 'مذهب', 'مذاهب', 'امام', 'ائمه', 'عید', 'اعیاد', 'رای', 'آرا', 'مصیبت', 'مصائب', 'اصل', 'اصول', 'عنصر', 'عناصر', 'رسم', 'رسوم', 'معبد', 'معابد', 'افق', 'آفاق', 'عاطفه', 'عواطف', 'رابطه', 'روابط', 'مسجد', 'مساجد', 'بیت', 'ابیات', 'عضو', 'اعضا', 'رمز', 'رموز', 'معبر', 'معابر', 'تاجر', 'تجار', 'عبارت', 'عبارات', 'رجل', 'رجال', 'مظهر', 'مظاهر', 'تصویر', 'تصاویر', 'عجیب', 'عجایب', 'رقم', 'ارقام', 'منظره', 'مناظر', 'جد', 'اجداد', 'فقیه', 'فقها', 'زاویه', 'زوایا', 'مرض', 'امراض', 'جانب', 'جوانب', 'فن', 'فنون', 'زعیم', 'زعما', 'مورد', 'موارد', 'جزیره', 'جزایر', 'فکر', 'افکار', 'سانحه', 'سوانح', 'مرحله', 'مراحل', 'جبل', 'جبال', 'فریضه', 'فرایض', 'سلطان', 'سلاطین', 'مفهوم', 'مفاهیم', 'جریمه', 'جرایم', 'فعل', 'افعال', 'شعر', 'اشعار', 'منبع', 'منابع', 'حادثه', 'حوادث', 'فقیر', 'فقرا', 'شاعر', 'شعرا', 'مکان', 'اماکن', 'حشم', 'احشام', 'قاعده', 'قواعد']\n",
    "mokassar_dict ={}\n",
    "for i in range(0,len(words_list),2):\n",
    "    mokassar_dict[words_list[i+1]] = words_list[i]\n",
    "                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(txt, seps):\n",
    "    default_sep = seps[0]    \n",
    "    # we skip seps[0] because that's the default separator\n",
    "    for sep in seps[1:]:\n",
    "        txt = txt.replace(sep, default_sep)\n",
    "    return [i.strip() for i in txt.split(default_sep)]\n",
    "\n",
    "#get doc and return (word,doc id)\n",
    "def tokenizer(text):\n",
    "    for l in english_char:\n",
    "        text = text.replace(l,\"\")\n",
    "    word_list = split(text.strip().replace(\"\\n\",\" \"), sep_list)\n",
    "    word_list = [x for x in word_list if not x.startswith(\"ir\") and not x.startswith(\"NID\")]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_marks(word_list):\n",
    "#     word_list = list(word_list)\n",
    "    for i in range(len(word_list)):\n",
    "        for mark in punctuation_marks:\n",
    "            word_list[i] = word_list[i].replace(mark,\"\")\n",
    "\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def edit_long_letters(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_long_letters]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_arabic_notation(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_arabic_notation]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def create_translation_table(src_list, dst_list):\n",
    "     return dict((ord(a), b) for a, b in zip(src_list, dst_list)) #map src unicode to dst char\n",
    "\n",
    "    \n",
    "def char_digit_Unification(word_list): \n",
    "    word_list = [x for x in word_list if not x.isnumeric() or (x.isnumeric() and float(x)<3000)]\n",
    "    translation_src, translation_dst = ' ىكي“”', ' یکی\"\"'\n",
    "    translation_src += 'ئ0123456789أإآ%'\n",
    "    translation_dst += 'ی۰۱۲۳۴۵۶۷۸۹ااا٪'\n",
    "    translations_table = create_translation_table(translation_src, translation_dst)\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i].translate(translations_table)\n",
    "    return word_list\n",
    "\n",
    "def remove_mokassar(word_list):\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = mokassar_dict.get(word_list[i],word_list[i])\n",
    "        \n",
    "    return word_list\n",
    "    \n",
    "\n",
    "\n",
    "def verb_Steaming(word_list):\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = all_verbs.get(word_list[i],word_list[i])\n",
    "        \n",
    "    return word_list\n",
    "    \n",
    "\n",
    "def remove_postfix(word_list):\n",
    "\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_postfix]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            if len(word_list[i])>4 and  not (word_list[i] in all_verbs_roots):\n",
    "                word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def remove_prefix(word_list):\n",
    "    starts =['بی‌' , 'نا', 'با']\n",
    "    for i in range(len(word_list)):\n",
    "        for start in starts:\n",
    "            if word_list[i].startswith(start) and len(word_list[i])>4:\n",
    "                word_list[i] = word_list[i][len(start):]\n",
    "                break\n",
    "\n",
    "    return word_list\n",
    "\n",
    "def morakab_Unification(word_list):\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_half_space]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    return word_list\n",
    "\n",
    "#all to gether \n",
    "def normalizer(word_list):\n",
    "    #remove punc\n",
    "    for i in range(len(word_list)):\n",
    "        for mark in punctuation_marks:\n",
    "            word_list[i] = word_list[i].replace(mark,\"\")\n",
    "            \n",
    "    #edit long letters\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_long_letters]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "            \n",
    "    #remove mokassar\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = mokassar_dict.get(word_list[i],word_list[i])\n",
    "    \n",
    "    #remove_arabic_notation\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_arabic_notation]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    \n",
    "    #char_digit_Unification\n",
    "    word_list = [x for x in word_list if not x.isnumeric() or (x.isnumeric() and float(x)<3000)]\n",
    "    translation_src, translation_dst = ' ىكي“”', ' یکی\"\"'\n",
    "    translation_src += 'ئ0123456789أإآ%'\n",
    "    translation_dst += 'ی۰۱۲۳۴۵۶۷۸۹ااا٪'\n",
    "    translations_table = create_translation_table(translation_src, translation_dst)\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = word_list[i].translate(translations_table)\n",
    "    \n",
    "    #verb_Steaming\n",
    "    for i in range(len(word_list)):\n",
    "        word_list[i] = all_verbs.get(word_list[i],word_list[i])\n",
    "        \n",
    "    #remove_prefix\n",
    "    starts =['بی‌' , 'نا', 'با']\n",
    "    for i in range(len(word_list)):\n",
    "        for start in starts:\n",
    "            if word_list[i].startswith(start) and len(word_list[i])>4:\n",
    "                word_list[i] = word_list[i][len(start):]\n",
    "                break\n",
    "    \n",
    "    #remove_postfix\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_postfix]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            if len(word_list[i])>4 and  not (word_list[i] in all_verbs_roots):\n",
    "                word_list[i] = pattern.sub(rep, word_list[i])\n",
    "    #morakab_Unification\n",
    "    patterns_compiled = [(re.compile(x[0]), x[1]) for x in raw_half_space]\n",
    "    for pattern, rep in patterns_compiled:\n",
    "        for i in range(len(word_list)):\n",
    "            word_list[i] = pattern.sub(rep, word_list[i])\n",
    "            \n",
    "    for word in word_list:\n",
    "        if word in stop_words:\n",
    "            word_list.remove(word)\n",
    "    \n",
    "    return [x for x in word_list if len(x) >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_data = pd.DataFrame()\n",
    "pd_list = []\n",
    "pd_list .append(pd.read_excel('IR00_3_11k News.xlsx', engine = 'openpyxl'))\n",
    "pd_list .append(pd.read_excel('IR00_3_17k News.xlsx', engine = 'openpyxl'))\n",
    "pd_list .append(pd.read_excel('IR00_3_20k News.xlsx', engine = 'openpyxl'))\n",
    "# pd_list .append(pd.read_excel('3.xlsx', engine = 'openpyxl'))\n",
    "# pd_list .append(pd.read_excel('2.xlsx', engine = 'openpyxl'))\n",
    "# pd_list .append(pd.read_excel('1.xlsx', engine = 'openpyxl'))\n",
    "for df in pd_list:\n",
    "    all_data = all_data.append(df,ignore_index=True)\n",
    "all_data[\"topic\"].replace({\"political\": \"politics\", \"sport\": \"sports\"}, inplace=True)\n",
    "all_data = all_data[all_data.content != 'None\\n\\n']\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "all_data.index +=1\n",
    "all_data['id'] = all_data.index\n",
    "contents = all_data['content'].to_list()\n",
    "doc_ids = all_data['id'].to_list()\n",
    "doc_url = all_data['url']\n",
    "docs_topic = all_data['topic']\n",
    "docs_num = len(doc_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create inverted index and change docs content to bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dictionary(contents, doc_ids):\n",
    "    for content, id in zip(contents,doc_ids):\n",
    "        doc_tokens = tokenizer(content)\n",
    "        doc_normalized_tokens = normalizer(doc_tokens)\n",
    "        term_freq_dict = dict(collections.Counter(doc_normalized_tokens))\n",
    "        \n",
    "        for term, freq in term_freq_dict.items():\n",
    "            term_freq_dict[term] = (1 + math.log((freq), 10))\n",
    "        contents[id-1] = term_freq_dict\n",
    "        for term,freq in term_freq_dict.items():\n",
    "            term_postings = inverted_index.get(term,{})\n",
    "            term_postings[id] = (1 + math.log((freq), 10))\n",
    "            inverted_index[term] = term_postings\n",
    "\n",
    "def calculate_tf_idf_weight(contents):\n",
    "    docs_len = []\n",
    "    for doc in contents:\n",
    "        length = 0\n",
    "        for term in doc.keys():\n",
    "            df = len(inverted_index[term])\n",
    "            idf = math.log(docs_num/df,10)\n",
    "            doc[term] *= idf\n",
    "            length += doc[term]\n",
    "        docs_len.append( math.sqrt(length))\n",
    "    return docs_len\n",
    "\n",
    "            \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inverted_index ={}\n",
    "create_dictionary(contents, doc_ids)\n",
    "docs_len = calculate_tf_idf_weight(contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save inverted_index and docs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.53 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# file_inverted_index = open('ph3_KNN_inverted_index.obj', 'wb') \n",
    "# pickle.dump(inverted_index, file_inverted_index)\n",
    "# file_inverted_index.close()\n",
    "# inverted_index = {}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_docs_len = open('ph3_KNN_docs_len.obj', 'wb') \n",
    "# pickle.dump(docs_len, file_docs_len)\n",
    "# file_docs_len.close()\n",
    "# docs_len = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_contents = open('ph3_KNN_contents.obj', 'wb') \n",
    "# pickle.dump(contents, file_contents)\n",
    "# file_contents.close()\n",
    "# contents =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inverted_index['سلام']\n",
    "# docs_len =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read inverted_index and docs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "readed = open('ph3_KNN_inverted_index.obj', 'rb') \n",
    "inverted_index = pickle.load(readed)\n",
    "readed = open('ph3_KNN_docs_len.obj', 'rb') \n",
    "docs_len = pickle.load(readed)\n",
    "readed = open('ph3_KNN_contents.obj', 'rb') \n",
    "contents = pickle.load(readed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_inverted_index = {}\n",
    "unlabeled_df = pd.read_excel('IR_Spring2021_ph12_7k.xlsx', engine = 'openpyxl')\n",
    "unlabeled_contents = unlabeled_df['content'].to_list()\n",
    "unlabeled_doc_ids = unlabeled_df['id'].to_list()\n",
    "unlabeled_doc_url = unlabeled_df['url']\n",
    "unlabeled_docs_num = len(doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create unlabeled inverted_index and update unlabeled_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_unlabaled_dictionary(unlabeled_contents, unlabeled_doc_ids):\n",
    "    for content, id in zip(unlabeled_contents,unlabeled_doc_ids):\n",
    "        doc_tokens = tokenizer(content)\n",
    "        doc_normalized_tokens = normalizer(doc_tokens)\n",
    "        term_freq_dict = dict(collections.Counter(doc_normalized_tokens))\n",
    "        \n",
    "        for term, freq in term_freq_dict.items():\n",
    "            term_freq_dict[term] = (1 + math.log((freq), 10))\n",
    "            \n",
    "        unlabeled_contents[id-1] = term_freq_dict\n",
    "        for term,freq in term_freq_dict.items():\n",
    "            term_postings = unlabeled_inverted_index.get(term,{})\n",
    "            term_postings[id] = freq\n",
    "            unlabeled_inverted_index[term] = term_postings\n",
    "            \n",
    "def calculate_tf_idf_weight(unlabeled_contents):\n",
    "    unlabeled_docs_len = []\n",
    "    for doc in unlabeled_contents:\n",
    "        length = 0\n",
    "        for term in doc.keys():\n",
    "            df = len(inverted_index[term])\n",
    "            idf = math.log(docs_num/df,10)\n",
    "            doc[term] *= idf\n",
    "            length += doc[term]\n",
    "        unlabeled_docs_len.append( math.sqrt(length))\n",
    "    return unlabeled_docs_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unlabeled_inverted_index = {}\n",
    "create_unlabaled_dictionary(unlabeled_contents, unlabeled_doc_ids)\n",
    "unlabeled_docs_len = calculate_tf_idf_weight(unlabeled_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 183 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_unlabeled_inverted_index = open('ph3_KNN_unlabeled_inverted_index.obj', 'wb') \n",
    "pickle.dump(unlabeled_inverted_index, file_unlabeled_inverted_index)\n",
    "file_unlabeled_inverted_index.close()\n",
    "unlabeled_inverted_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_unlabeled_docs_len = open('ph3_KNN_unlabeled_docs_len.obj', 'wb') \n",
    "pickle.dump(unlabeled_docs_len, file_unlabeled_docs_len)\n",
    "file_unlabeled_docs_len.close()\n",
    "unlabeled_docs_len = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_unlabeled_contents = open('ph3_KNN_unlabeled_contents.obj', 'wb') \n",
    "pickle.dump(unlabeled_contents, file_unlabeled_contents)\n",
    "file_unlabeled_contents.close()\n",
    "unlabeled_contents =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read unlabeled inverted_index and contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 625 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "readed = open('ph3_KNN_unlabeled_inverted_index.obj', 'rb') \n",
    "unlabeled_inverted_index = pickle.load(readed)\n",
    "readed = open('ph3_KNN_unlabeled_docs_len.obj', 'rb') \n",
    "unlabeled_docs_len = pickle.load(readed)\n",
    "readed = open('ph3_KNN_unlabeled_contents.obj', 'rb') \n",
    "unlabeled_contents = pickle.load(readed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_k(doc_terms, k):\n",
    "    similarity = {}\n",
    "    docs_score = [0] * docs_num\n",
    "    for term,Wt_q in doc_terms.items():  \n",
    "        idf = math.log(docs_num/len(inverted_index[term]))\n",
    "        for doc_id, freq in inverted_index[term].items():    \n",
    "            docs_score[doc_id-1] += (Wt_q * freq)*idf\n",
    "      \n",
    "    docs_score = list((np.array(docs_score))/(np.array(docs_len)))\n",
    "    for doc_id in doc_ids:\n",
    "        if docs_score[doc_id-1] != 0:\n",
    "            similarity[doc_id] = docs_score[doc_id-1]\n",
    "    heap = [(-value, key) for key,value in similarity.items()]\n",
    "    largest = heapq.nsmallest(k, heap)\n",
    "    top_k = [(y, -x) for x,y in largest]\n",
    "    return top_k\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(unlabeled_doc_ids,unlabeled_contents, k ):\n",
    "    categories = {\"culture\":[], \"economy\":[], \"sports\":[], \"politics\":[], \"health\":[]}\n",
    "    for doc_id in tqdm(unlabeled_doc_ids):\n",
    "        doc_terms = unlabeled_contents[doc_id-1]\n",
    "        top_k = extract_top_k(doc_terms, k)\n",
    "        labels = [docs_topic[x] for x,_ in top_k]\n",
    "        categories[max(set(labels), key=labels.count)].append(doc_id)\n",
    "    return categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/7000 [00:00<?, ?it/s]C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if __name__ == '__main__':\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 7000/7000 [22:02<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cat = KNN(unlabeled_doc_ids, unlabeled_contents, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "culture    179\n",
      "economy    2019\n",
      "sports    1695\n",
      "politics    1632\n",
      "health    1475\n"
     ]
    }
   ],
   "source": [
    "for categ in cat:\n",
    "    print(categ,\"  \", len(cat[categ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save labeled docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# file_labeled_7kdocs = open('ph3_labeled_7kdocs.obj', 'wb') \n",
    "# pickle.dump(cat, file_labeled_7kdocs)\n",
    "# file_labeled_7kdocs.close()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load labeled docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "readed = open('ph3_labeled_7kdocs.obj', 'rb') \n",
    "categories = pickle.load(readed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query responding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_query(query_str):\n",
    "    query_tokens = tokenizer(query_str)\n",
    "    query_normalized_tokens = normalizer(query_tokens)\n",
    "    term_freq_dict = dict(collections.Counter(query_normalized_tokens))\n",
    "    query_terms = {}\n",
    "    for  term,freq in term_freq_dict.items():\n",
    "        if term in inverted_index.keys() and term not in stop_words:\n",
    "            query_terms[term] = 1+math.log(freq,10)\n",
    "    print(query_terms)\n",
    "    return query_terms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_docs(query_terms, cat_docs):\n",
    "    docs_score = {x:0 for x in cat_docs}\n",
    "    for term,Wt_q in query_terms.items():\n",
    "        for doc_id in cat_docs:\n",
    "            docs_score[doc_id] += Wt_q * unlabeled_contents[doc_id-1].get(term, 0)\n",
    "    \n",
    "    for doc_id in cat_docs:\n",
    "        docs_score[doc_id] /= unlabeled_docs_len[doc_id-1]\n",
    "    return docs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_k(docs_similarity, k):\n",
    "\n",
    "    heap = [(-value, key) for key,value in docs_similarity.items()]\n",
    "    largest = heapq.nsmallest(k, heap)\n",
    "    top_k = [(y, -x) for x,y in largest]\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_responding():\n",
    "    query = input(\"chose one category between [culture, economy, sports, politics, health] \\n enter query like this cat:text : \")\n",
    "    category = query.split(':')[0]\n",
    "    query = query.split(':')[1]\n",
    "\n",
    "    query_terms = vectorize_query(query)\n",
    "    \n",
    "    cat_docs = categories[category]\n",
    "    docs_similarity = score_docs(query_terms, cat_docs)\n",
    "    a = datetime.datetime.now()\n",
    "    top_docs = extract_top_k(docs_similarity, 50)\n",
    "    b = datetime.datetime.now()\n",
    "\n",
    "    print(\"{:<5} result in {} ms\\nid \\t(score)\\t\\t  -> link\\n\".format(len(top_docs),1000*(b-a).total_seconds()))\n",
    "    for doc_id, score in top_docs:\n",
    "        if score>0:\n",
    "            print( \"{:<5}({}) -> {} \".format(doc_id,\"-\", doc_url[doc_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chose one category between [culture, economy, sports, politics, health] \n",
      " enter query like this cat:text : economy:انقلاب\n",
      "{'انقلاب': 1.0}\n",
      "50    result in 0.0 ms\n",
      "id \t(score)\t\t  -> link\n",
      "\n",
      "4423 (-) -> https://www.isna.ir/news/98021306870/طول-خطوط-ریلی-ایران-امسال-چقدر-می-شود \n",
      "3302 (-) -> https://www.isna.ir/news/99020906760/اعتبار-۷۰-میلیاردی-توسعه-زیرساخت-های-گردشگری-روستاهای-استان-بوشهر \n",
      "1983 (-) -> https://www.isna.ir/news/99052317223/پرونده-فرودگاه-قدیم-غار-نمکدان-و-پلاژ-بانوان-قشم-روی-میز-دادستان \n",
      "5276 (-) -> https://www.isna.ir/news/98110604396/10-هزار-واحد-در-طرح-اقدام-ملی-مسکن-در-قم-پیش-بینی-شده-است \n",
      "4186 (-) -> https://www.isna.ir/news/99111511732/۲۶پروژه-با-۲۳۰۰میلیارد-تومان-در-کردستان-افتتاح-می-شود \n",
      "5277 (-) -> https://www.isna.ir/news/98110604329/کلنگ-4000-واحد-مسکونی-طرح-اقدام-ملی-در-قم-بر-زمین-زده-شد \n",
      "2560 (-) -> https://www.isna.ir/news/98020301754/مخبر-دشمن-با-روح-حکومت-کار-دارد \n",
      "5275 (-) -> https://www.isna.ir/news/98110604422/آیین-کلنگ-زنی-۴۰۰۰-واحد-مسکونی-طرح-اقدام-ملی-در-استان-قم \n",
      "4504 (-) -> https://www.isna.ir/news/98022714140/اختصاص-۲۵۰۰-میلیارد-تومان-وام-قرض-الحسنه-ساخت-و-تعمیر-مسکن-برای \n",
      "5298 (-) -> https://www.isna.ir/news/98111107354/99-5-درصد-از-استان-قم-تحت-پوشش-گازرسانی-قرار-دارد \n",
      "4360 (-) -> https://www.isna.ir/news/98012711914/خسارت-سیل-به-۱۴۱-روستا-و-شهر-استان-یزد \n",
      "4159 (-) -> https://www.isna.ir/news/99110503344/کدام-بخش-های-اقتصاد-پس-از-کرونا-اوج-می-گیرند \n",
      "3503 (-) -> https://www.isna.ir/news/99040403412/شیب-کاهشی-واردات-کالا-از-گمرکات-بوشهر \n",
      "4326 (-) -> https://www.isna.ir/news/98011907014/القای-جو-روانی-برای-افزایش-قیمت-ارز \n",
      "2587 (-) -> https://www.isna.ir/news/98021005645/عارف-بیشترین-فشار-اقتصادی-متوجه-کارگران-است \n",
      "5121 (-) -> https://www.isna.ir/news/98090704741/هر-کدام-از-تحریم-های-اقتصادی-می-تواند-دولتی-را-سرنگون-کند \n",
      "4409 (-) -> https://www.isna.ir/news/98020804058/چرا-تعاونی-ها-قدرت-قدیم-را-ندارند \n",
      "3740 (-) -> https://www.isna.ir/news/99061108330/تقویت-روابط-تجاری-ایران-و-کره-شمالی \n",
      "3219 (-) -> https://www.isna.ir/news/99011607964/پالایش-متقاضیان-اینترنتی-مسکن-ملی-آغاز-شد \n",
      "3520 (-) -> https://www.isna.ir/news/99041309682/تولید-L۹۰-ایرانی-در-حال-پیگیری-است \n",
      "2996 (-) -> https://www.isna.ir/news/98082717828/نمی-خواهیم-در-این-شرایط-چماق-روی-سر-افراد-بگیریم \n",
      "2762 (-) -> https://www.isna.ir/news/98050502261/طلبکاران-پدیده-به-سهامداران-تبدیل-شدند \n",
      "3840 (-) -> https://www.isna.ir/news/99071712978/روان-بخشی-روستاها-با-طرح-هادی \n",
      "5222 (-) -> https://www.isna.ir/news/98100906510/مصوبه-دولت-در-تامین-80-واگن-پیشبرد-متروی-قم-را-تسهیل-کرد \n",
      "5160 (-) -> https://www.isna.ir/news/98092015527/انعقاد-قرارداد-داخلی-سازی-قطعات-صنعت-خودرو-به-ارزش-۲۷۵-میلیون \n",
      "4148 (-) -> https://www.isna.ir/news/99110201450/خودروی-داخلی-تا-۳۰-درصد-افت-قیمت-داشته-است \n",
      "3387 (-) -> https://www.isna.ir/news/99022920827/سهم-۶-۵-درصدی-اقتصاد-دیجیتال-از-تولید-ناخالص-داخلی \n",
      "4552 (-) -> https://www.isna.ir/news/98030803635/جاده-ای-که-از-سال-۶۲-تاکنون-نیمه-کاره-مانده-است \n",
      "4821 (-) -> https://www.isna.ir/news/98052713445/امروز-بازار-انباشته-از-اقلام-مختلف-است \n",
      "4874 (-) -> https://www.isna.ir/news/98061106058/اجرای-طرح-یارانه-دستمزد-گشایشی-جدید-در-امر-اشتغال \n",
      "4087 (-) -> https://www.isna.ir/news/99101008022/مقاوم-سازی-۴۳-هزار-و-۸۹۰-واحد-در-سطح-استان-بوشهر \n",
      "4048 (-) -> https://www.isna.ir/news/99093023274/برای-تکمیل-موجودی-مسکن-ملی-فقط-امروز-فرصت-دارید \n",
      "4847 (-) -> https://www.isna.ir/news/98060301355/تفاوت-هیوندا-و-ایران-خودرو-از-نگاه-عضو-اتاق-بازرگانی-مشهد \n",
      "4624 (-) -> https://www.isna.ir/news/98033014767/ارائه-طرح-اصلاح-ساختار-بودجه-کشور-در-جلسه-سران-قوا \n",
      "5290 (-) -> https://www.isna.ir/news/98110705275/تاج-گردون-حقوق-یک-میلیون-و-۱۰۰-هزار-کارمند-در-سال-آینده-۸۰-درصد \n",
      "4615 (-) -> https://www.isna.ir/news/98032813583/آغاز-نهضت-خانه-سازی-در-وزارت-راه-و-شهرسازی \n",
      "1933 (-) -> https://www.isna.ir/news/99050402405/آمریکایی-ها-قصد-سرنگونی-هواپیمای-مسافربری-ایران-را-داشتند \n",
      "4914 (-) -> https://www.isna.ir/news/98062009860/66-درصد-دشت-های-ایران-ممنوعه-اند-10-هزار-روستا-آب-پایدار-ندارند \n",
      "4418 (-) -> https://www.isna.ir/news/98021106309/امکان-تخصیص-۵-میلیارد-تومان-به-پروژه-های-آب-رسانی-عشایر-قزوین \n",
      "4713 (-) -> https://www.isna.ir/news/98042613967/سهم-ناچیز-ایران-از-بازار-۲۵۰۰-میلیارد-دلاری-صادرات-حلال \n",
      "4873 (-) -> https://www.isna.ir/news/98061005228/برندهای-بین-المللی-کشور-انگشت-شمار-است \n",
      "4913 (-) -> https://www.isna.ir/news/98062009861/تنها-5-درصد-قطارهای-ایران-پنج-ستاره-اند \n",
      "4995 (-) -> https://www.isna.ir/news/98071612661/انجمن-های-صنفی-خواستار-توقف-تصویب-لایحه-تجارت-شدند \n",
      "4726 (-) -> https://www.isna.ir/news/98043015879/اولین-مطالبه-اتاق-ایران-شفافیت-و-مقابله-با-فساد-است \n",
      "2681 (-) -> https://www.isna.ir/news/98040100590/کاهش-۱۴-درصدی-آمار-ازدواج-و-طلاق-در-استان-مرکزی \n",
      "5300 (-) -> https://www.isna.ir/news/98111208137/ایجاد-2074-شغل-ره-آورد-افتتاح-پروژه-های-دهه-فجر-98-قم \n",
      "5380 (-) -> https://www.isna.ir/news/98121814020/آغاز-ثبت-نام-مسکن-ملی-در-۱۷-استان-از-فردا-شروط-جدیدی-اضافه-شد \n",
      "5136 (-) -> https://www.isna.ir/news/98091208462/با-سیاسی-کردن-تراریخته-ها-پیشرفت-علمی-کشور-ذبح-می-شود-بحث-علیه \n"
     ]
    }
   ],
   "source": [
    "query_responding()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
